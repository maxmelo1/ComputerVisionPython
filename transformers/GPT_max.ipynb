{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Material do curso de nivelamento em Deep Learning do professor Edson M. Takashi (FACOM - UFMS)"
      ],
      "metadata": {
        "id": "w2HHrcgvOcFb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0hsG1rk6xqf",
        "outputId": "b54471d7-ab17-461f-efe1-e1a96131ad39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package machado to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('machado')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import machado"
      ],
      "metadata": {
        "id": "v5ZXNPzBMJVY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=machado.raw()"
      ],
      "metadata": {
        "id": "JAQGKRjPMOvP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "735xpHSzMR2T",
        "outputId": "8e6523b5-f799-474c-835d-5c4b26c83088"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conto, Contos Fluminenses, 1870\n",
            "\n",
            "Contos Fluminenses\n",
            "\n",
            "Texto-fonte:\n",
            "\n",
            "Obra Completa, Machado de Assis, vol. II,\n",
            "\n",
            "Rio de Janeiro: Nova Aguilar, 1994.\n",
            "\n",
            "Publicado originalmente pela\n",
            "Editora Garnier, Rio de Janeiro, em 1870.\n",
            "\n",
            "ÍNDICE\n",
            "\n",
            "MISS DOLLAR\n",
            "\n",
            "LUÍS\n",
            "SOARES\n",
            "\n",
            "A MULHER DE\n",
            "PRETO\n",
            "\n",
            "O\n",
            "SEGREDO DE AUGUSTA\n",
            "\n",
            "CONFISSÕES DE UMA VIÚVA MOÇA\n",
            "\n",
            "LINHA\n",
            "RETA E LINHA CURVA\n",
            "\n",
            "FREI\n",
            "SIMÃO\n",
            "\n",
            "MISS\n",
            "DOLLAR\n",
            "\n",
            "ÍNDICE\n",
            "\n",
            "Capítulo Primeiro\n",
            "\n",
            "Capítulo II\n",
            "\n",
            "Capítulo iii\n",
            "\n",
            "Capítulo iv\n",
            "\n",
            "Capítulo v\n",
            "\n",
            "Capítulo vI\n",
            "\n",
            "Capítulo vII\n",
            "\n",
            "CAPÍTULO VIII\n",
            "\n",
            "CAPÍTULO PRIMEIRO\n",
            "\n",
            "Era conveniente ao romance que o leitor\n",
            "ficasse muito tempo sem saber quem era Miss Dollar. Mas por outro lado,\n",
            "sem a apresentação de Miss Dollar, seria o autor obrigado a longas\n",
            "digressões, que encheriam o papel sem adiantar a ação. Não há hesitação\n",
            "possível: vou apresentar-lhes Miss Dollar.\n",
            "\n",
            "Se o leitor é rapaz e dado ao gênio\n",
            "melancólico, imagina que Miss Dollar é uma inglesa pálida e delgada,\n",
            "escassa de carnes e de sangue, abrindo à flor do rosto dois grandes olhos azuis\n",
            "e sac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZOoy43uMYyA",
        "outputId": "965b1327-9ee4-4e7f-c1cd-8ef1f07090c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            " !\"$%&'()*+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_`abcdefghijklmnopqrstuvwxyz ¡§ª«­°´º»½¿ÀÁÂÃÇÈÉÊËÍÓÔÕÚÛÜàáâãäçèéêëìíîïñòóôõöùúûü\n",
            "147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"olá isso é um teste\"))\n",
        "print(decode(encode(\"olá isso é um teste\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM7wbMboMb58",
        "outputId": "1556ba09-a70a-4981-b59a-e8df1ba17c0b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[75, 72, 124, 2, 69, 79, 79, 75, 2, 130, 2, 81, 73, 2, 80, 65, 79, 80, 65]\n",
            "olá isso é um teste\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "itos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zMs3DxwMexL",
        "outputId": "116faddc-c884-420b-81b0-749dd9bc3625"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '\\t',\n",
              " 1: '\\n',\n",
              " 2: ' ',\n",
              " 3: '!',\n",
              " 4: '\"',\n",
              " 5: '$',\n",
              " 6: '%',\n",
              " 7: '&',\n",
              " 8: \"'\",\n",
              " 9: '(',\n",
              " 10: ')',\n",
              " 11: '*',\n",
              " 12: '+',\n",
              " 13: ',',\n",
              " 14: '-',\n",
              " 15: '.',\n",
              " 16: '/',\n",
              " 17: '0',\n",
              " 18: '1',\n",
              " 19: '2',\n",
              " 20: '3',\n",
              " 21: '4',\n",
              " 22: '5',\n",
              " 23: '6',\n",
              " 24: '7',\n",
              " 25: '8',\n",
              " 26: '9',\n",
              " 27: ':',\n",
              " 28: ';',\n",
              " 29: '=',\n",
              " 30: '?',\n",
              " 31: 'A',\n",
              " 32: 'B',\n",
              " 33: 'C',\n",
              " 34: 'D',\n",
              " 35: 'E',\n",
              " 36: 'F',\n",
              " 37: 'G',\n",
              " 38: 'H',\n",
              " 39: 'I',\n",
              " 40: 'J',\n",
              " 41: 'K',\n",
              " 42: 'L',\n",
              " 43: 'M',\n",
              " 44: 'N',\n",
              " 45: 'O',\n",
              " 46: 'P',\n",
              " 47: 'Q',\n",
              " 48: 'R',\n",
              " 49: 'S',\n",
              " 50: 'T',\n",
              " 51: 'U',\n",
              " 52: 'V',\n",
              " 53: 'W',\n",
              " 54: 'X',\n",
              " 55: 'Y',\n",
              " 56: 'Z',\n",
              " 57: '[',\n",
              " 58: ']',\n",
              " 59: '_',\n",
              " 60: '`',\n",
              " 61: 'a',\n",
              " 62: 'b',\n",
              " 63: 'c',\n",
              " 64: 'd',\n",
              " 65: 'e',\n",
              " 66: 'f',\n",
              " 67: 'g',\n",
              " 68: 'h',\n",
              " 69: 'i',\n",
              " 70: 'j',\n",
              " 71: 'k',\n",
              " 72: 'l',\n",
              " 73: 'm',\n",
              " 74: 'n',\n",
              " 75: 'o',\n",
              " 76: 'p',\n",
              " 77: 'q',\n",
              " 78: 'r',\n",
              " 79: 's',\n",
              " 80: 't',\n",
              " 81: 'u',\n",
              " 82: 'v',\n",
              " 83: 'w',\n",
              " 84: 'x',\n",
              " 85: 'y',\n",
              " 86: 'z',\n",
              " 87: '\\x85',\n",
              " 88: '\\x91',\n",
              " 89: '\\x92',\n",
              " 90: '\\x93',\n",
              " 91: '\\x94',\n",
              " 92: '\\x96',\n",
              " 93: '\\x97',\n",
              " 94: '\\x9c',\n",
              " 95: '\\xa0',\n",
              " 96: '¡',\n",
              " 97: '§',\n",
              " 98: 'ª',\n",
              " 99: '«',\n",
              " 100: '\\xad',\n",
              " 101: '°',\n",
              " 102: '´',\n",
              " 103: 'º',\n",
              " 104: '»',\n",
              " 105: '½',\n",
              " 106: '¿',\n",
              " 107: 'À',\n",
              " 108: 'Á',\n",
              " 109: 'Â',\n",
              " 110: 'Ã',\n",
              " 111: 'Ç',\n",
              " 112: 'È',\n",
              " 113: 'É',\n",
              " 114: 'Ê',\n",
              " 115: 'Ë',\n",
              " 116: 'Í',\n",
              " 117: 'Ó',\n",
              " 118: 'Ô',\n",
              " 119: 'Õ',\n",
              " 120: 'Ú',\n",
              " 121: 'Û',\n",
              " 122: 'Ü',\n",
              " 123: 'à',\n",
              " 124: 'á',\n",
              " 125: 'â',\n",
              " 126: 'ã',\n",
              " 127: 'ä',\n",
              " 128: 'ç',\n",
              " 129: 'è',\n",
              " 130: 'é',\n",
              " 131: 'ê',\n",
              " 132: 'ë',\n",
              " 133: 'ì',\n",
              " 134: 'í',\n",
              " 135: 'î',\n",
              " 136: 'ï',\n",
              " 137: 'ñ',\n",
              " 138: 'ò',\n",
              " 139: 'ó',\n",
              " 140: 'ô',\n",
              " 141: 'õ',\n",
              " 142: 'ö',\n",
              " 143: 'ù',\n",
              " 144: 'ú',\n",
              " 145: 'û',\n",
              " 146: 'ü'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NCZTHB3MimA",
        "outputId": "579de406-6327-4827-8fba-b6fdeb1d8e6f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([14840456]) torch.int64\n",
            "tensor([ 33,  75,  74,  80,  75,  13,   2,  33,  75,  74,  80,  75,  79,   2,\n",
            "         36,  72,  81,  73,  69,  74,  65,  74,  79,  65,  79,  13,   2,  18,\n",
            "         25,  24,  17,   1,   1,  33,  75,  74,  80,  75,  79,   2,  36,  72,\n",
            "         81,  73,  69,  74,  65,  74,  79,  65,  79,   1,   1,  50,  65,  84,\n",
            "         80,  75,  14,  66,  75,  74,  80,  65,  27,   1,   1,  45,  62,  78,\n",
            "         61,   2,  33,  75,  73,  76,  72,  65,  80,  61,  13,   2,  43,  61,\n",
            "         63,  68,  61,  64,  75,   2,  64,  65,   2,  31,  79,  79,  69,  79,\n",
            "         13,   2,  82,  75,  72,  15,   2,  39,  39,  13,   1,   1,  48,  69,\n",
            "         75,   2,  64,  65,   2,  40,  61,  74,  65,  69,  78,  75,  27,   2,\n",
            "         44,  75,  82,  61,   2,  31,  67,  81,  69,  72,  61,  78,  13,   2,\n",
            "         18,  26,  26,  21,  15,   1,   1,  46,  81,  62,  72,  69,  63,  61,\n",
            "         64,  75,   2,  75,  78,  69,  67,  69,  74,  61,  72,  73,  65,  74,\n",
            "         80,  65,   2,  76,  65,  72,  61,   1,  35,  64,  69,  80,  75,  78,\n",
            "         61,   2,  37,  61,  78,  74,  69,  65,  78,  13,   2,  48,  69,  75,\n",
            "          2,  64,  65,   2,  40,  61,  74,  65,  69,  78,  75,  13,   2,  65,\n",
            "         73,   2,  18,  25,  24,  17,  15,   1,   1, 116,  44,  34,  39,  33,\n",
            "         35,   1,   1,  43,  39,  49,  49,   2,  34,  45,  42,  42,  31,  48,\n",
            "          1,   1,  42,  51, 116,  49,   1,  49,  45,  31,  48,  35,  49,   1,\n",
            "          1,  31,   2,  43,  51,  42,  38,  35,  48,   2,  34,  35,   1,  46,\n",
            "         48,  35,  50,  45,   1,   1,  45,   1,  49,  35,  37,  48,  35,  34,\n",
            "         45,   2,  34,  35,   2,  31,  51,  37,  51,  49,  50,  31,   1,   1,\n",
            "         33,  45,  44,  36,  39,  49,  49, 119,  35,  49,   2,  34,  35,   2,\n",
            "         51,  43,  31,   2,  52,  39, 120,  52,  31,   2,  43,  45, 111,  31,\n",
            "          1,   1,  42,  39,  44,  38,  31,   1,  48,  35,  50,  31,   2,  35,\n",
            "          2,  42,  39,  44,  38,  31,   2,  33,  51,  48,  52,  31,   1,   1,\n",
            "         36,  48,  35,  39,   1,  49,  39,  43, 110,  45,   1,   1,  43,  39,\n",
            "         49,  49,   1,  34,  45,  42,  42,  31,  48,   1,   1, 116,  44,  34,\n",
            "         39,  33,  35,   1,   1,  33,  61,  76, 134,  80,  81,  72,  75,   2,\n",
            "         46,  78,  69,  73,  65,  69,  78,  75,   1,   1,  33,  61,  76, 134,\n",
            "         80,  81,  72,  75,   2,  39,  39,   1,   1,  33,  61,  76, 134,  80,\n",
            "         81,  72,  75,   2,  69,  69,  69,   1,   1,  33,  61,  76, 134,  80,\n",
            "         81,  72,  75,   2,  69,  82,   1,   1,  33,  61,  76, 134,  80,  81,\n",
            "         72,  75,   2,  82,   1,   1,  33,  61,  76, 134,  80,  81,  72,  75,\n",
            "          2,  82,  39,   1,   1,  33,  61,  76, 134,  80,  81,  72,  75,   2,\n",
            "         82,  39,  39,   1,   1,  33,  31,  46, 116,  50,  51,  42,  45,   2,\n",
            "         52,  39,  39,  39,   1,   1,  33,  31,  46, 116,  50,  51,  42,  45,\n",
            "          2,  46,  48,  39,  43,  35,  39,  48,  45,   1,   1,  35,  78,  61,\n",
            "          2,  63,  75,  74,  82,  65,  74,  69,  65,  74,  80,  65,   2,  61,\n",
            "         75,   2,  78,  75,  73,  61,  74,  63,  65,   2,  77,  81,  65,   2,\n",
            "         75,   2,  72,  65,  69,  80,  75,  78,   1,  66,  69,  63,  61,  79,\n",
            "         79,  65,   2,  73,  81,  69,  80,  75,   2,  80,  65,  73,  76,  75,\n",
            "          2,  79,  65,  73,   2,  79,  61,  62,  65,  78,   2,  77,  81,  65,\n",
            "         73,   2,  65,  78,  61,   2,  43,  69,  79,  79,   2,  34,  75,  72,\n",
            "         72,  61,  78,  15,   2,  43,  61,  79,   2,  76,  75,  78,   2,  75,\n",
            "         81,  80,  78,  75,   2,  72,  61,  64,  75,  13,   1,  79,  65,  73,\n",
            "          2,  61,   2,  61,  76,  78,  65,  79,  65,  74,  80,  61, 128, 126,\n",
            "         75,   2,  64,  65,   2,  43,  69,  79,  79,   2,  34,  75,  72,  72,\n",
            "         61,  78,  13,   2,  79,  65,  78,  69,  61,   2,  75,   2,  61,  81,\n",
            "         80,  75,  78,   2,  75,  62,  78,  69,  67,  61,  64,  75,   2,  61,\n",
            "          2,  72,  75,  74,  67,  61,  79,   1,  64,  69,  67,  78,  65,  79,\n",
            "         79, 141,  65,  79,  13,   2,  77,  81,  65,   2,  65,  74,  63,  68,\n",
            "         65,  78,  69,  61,  73,   2,  75,   2,  76,  61,  76,  65,  72,   2,\n",
            "         79,  65,  73,   2,  61,  64,  69,  61,  74,  80,  61,  78,   2,  61,\n",
            "          2,  61, 128, 126,  75,  15,   2,  44, 126,  75,   2,  68, 124,   2,\n",
            "         68,  65,  79,  69,  80,  61, 128, 126,  75,   1,  76,  75,  79,  79,\n",
            "        134,  82,  65,  72,  27,   2,  82,  75,  81,   2,  61,  76,  78,  65,\n",
            "         79,  65,  74,  80,  61,  78,  14,  72,  68,  65,  79,   2,  43,  69,\n",
            "         79,  79,   2,  34,  75,  72,  72,  61,  78,  15,   1,   1,  49,  65,\n",
            "          2,  75,   2,  72,  65,  69,  80,  75,  78,   2, 130,   2,  78,  61,\n",
            "         76,  61,  86,   2,  65,   2,  64,  61,  64,  75,   2,  61,  75,   2,\n",
            "         67, 131,  74,  69,  75,   1,  73,  65,  72,  61,  74,  63, 139,  72,\n",
            "         69,  63,  75,  13,   2,  69,  73,  61,  67,  69,  74,  61,   2,  77,\n",
            "         81,  65,   2,  43,  69,  79,  79,   2,  34,  75,  72,  72,  61,  78,\n",
            "          2, 130,   2,  81,  73,  61,   2,  69,  74,  67,  72,  65,  79,  61,\n",
            "          2,  76, 124,  72,  69,  64,  61,   2,  65,   2,  64,  65,  72,  67,\n",
            "         61,  64,  61,  13,   1,  65,  79,  63,  61,  79,  79,  61,   2,  64,\n",
            "         65,   2,  63,  61,  78,  74,  65,  79,   2,  65,   2,  64,  65,   2,\n",
            "         79,  61,  74,  67,  81,  65,  13,   2,  61,  62,  78,  69,  74,  64,\n",
            "         75,   2, 123,   2,  66,  72,  75,  78,   2,  64,  75,   2,  78,  75,\n",
            "         79,  80,  75,   2,  64,  75,  69,  79,   2,  67,  78,  61,  74,  64,\n",
            "         65,  79,   2,  75,  72,  68,  75,  79,   2,  61,  86,  81,  69,  79,\n",
            "          1,  65,   2,  79,  61,  63])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "gcU0MOizMnc_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVTy5FURMwY4",
        "outputId": "b6df3ead-4cb6-4176-8625-66760fded297"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([33, 75, 74, 80, 75, 13,  2, 33, 75])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQXYxE1gMybw",
        "outputId": "5b99fd4c-3ae6-4fb2-b427-ed9a09f02ff2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([33]) the target: 75\n",
            "when input is tensor([33, 75]) the target: 74\n",
            "when input is tensor([33, 75, 74]) the target: 80\n",
            "when input is tensor([33, 75, 74, 80]) the target: 75\n",
            "when input is tensor([33, 75, 74, 80, 75]) the target: 13\n",
            "when input is tensor([33, 75, 74, 80, 75, 13]) the target: 2\n",
            "when input is tensor([33, 75, 74, 80, 75, 13,  2]) the target: 33\n",
            "when input is tensor([33, 75, 74, 80, 75, 13,  2, 33]) the target: 75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPcxQoCRM1RS",
        "outputId": "88321699-b8fe-4a28-b718-196eb7b3d194"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[76, 78, 65, 65, 74, 64, 65, 81],\n",
            "        [78, 73, 69, 80, 65, 13,  2, 82],\n",
            "        [65, 67, 75, 63, 69, 61, 74, 80],\n",
            "        [75, 73, 65, 81,  2, 65,  2, 75]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[78, 65, 65, 74, 64, 65, 81,  2],\n",
            "        [73, 69, 80, 65, 13,  2, 82, 69],\n",
            "        [67, 75, 63, 69, 61, 74, 80, 65],\n",
            "        [73, 65, 81,  2, 65,  2, 75, 72]])\n",
            "----\n",
            "when input is [76] the target: 78\n",
            "when input is [76, 78] the target: 65\n",
            "when input is [76, 78, 65] the target: 65\n",
            "when input is [76, 78, 65, 65] the target: 74\n",
            "when input is [76, 78, 65, 65, 74] the target: 64\n",
            "when input is [76, 78, 65, 65, 74, 64] the target: 65\n",
            "when input is [76, 78, 65, 65, 74, 64, 65] the target: 81\n",
            "when input is [76, 78, 65, 65, 74, 64, 65, 81] the target: 2\n",
            "when input is [78] the target: 73\n",
            "when input is [78, 73] the target: 69\n",
            "when input is [78, 73, 69] the target: 80\n",
            "when input is [78, 73, 69, 80] the target: 65\n",
            "when input is [78, 73, 69, 80, 65] the target: 13\n",
            "when input is [78, 73, 69, 80, 65, 13] the target: 2\n",
            "when input is [78, 73, 69, 80, 65, 13, 2] the target: 82\n",
            "when input is [78, 73, 69, 80, 65, 13, 2, 82] the target: 69\n",
            "when input is [65] the target: 67\n",
            "when input is [65, 67] the target: 75\n",
            "when input is [65, 67, 75] the target: 63\n",
            "when input is [65, 67, 75, 63] the target: 69\n",
            "when input is [65, 67, 75, 63, 69] the target: 61\n",
            "when input is [65, 67, 75, 63, 69, 61] the target: 74\n",
            "when input is [65, 67, 75, 63, 69, 61, 74] the target: 80\n",
            "when input is [65, 67, 75, 63, 69, 61, 74, 80] the target: 65\n",
            "when input is [75] the target: 73\n",
            "when input is [75, 73] the target: 65\n",
            "when input is [75, 73, 65] the target: 81\n",
            "when input is [75, 73, 65, 81] the target: 2\n",
            "when input is [75, 73, 65, 81, 2] the target: 65\n",
            "when input is [75, 73, 65, 81, 2, 65] the target: 2\n",
            "when input is [75, 73, 65, 81, 2, 65, 2] the target: 75\n",
            "when input is [75, 73, 65, 81, 2, 65, 2, 75] the target: 72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcctV7mwM8y-",
        "outputId": "53dd0751-a4c2-42be-994d-645dc7624534"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[76, 78, 65, 65, 74, 64, 65, 81],\n",
            "        [78, 73, 69, 80, 65, 13,  2, 82],\n",
            "        [65, 67, 75, 63, 69, 61, 74, 80],\n",
            "        [75, 73, 65, 81,  2, 65,  2, 75]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOr4JuZbNFUL",
        "outputId": "59902b34-b4ab-4e9b-912e-f2ca4084881a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 147])\n",
            "tensor(5.6646, grad_fn=<NllLossBackward0>)\n",
            "\tÉ\n",
            "/EiKÕ0'´-­ÇôY3\tZ­RáÂ\n",
            "1?f¡áe¡&rWC9[Â)Ff¡É:=üÇa\tÀ­ÊtS¡qñf«8õ«¿Ûr«ç_AGa°à47õÔÔ¿ÇöïªeÛtÜ\n",
            "Lâ°íRP\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKasWx6jNJgt",
        "outputId": "f20a7a52-4fe0-453a-d8bb-539a9f2402bb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,  31,  66, 110,  12,  18,  44, 102,  94,  96, 113, 131,  31,  14,\n",
              "          39,  56,  74,  57,  46, 133,  56,  91,   6, 144,  66,  18,  71,   6,\n",
              "          92,  42,  71, 123, 143,   9,  97, 135,  11, 132,  42,  29,  66, 123,\n",
              "          32,  21,  63,  50,  81,  86,  77, 135,  64,  51,  23,  37, 133,  75,\n",
              "          64,  94,  79, 127, 133,  25,   8,  26,  22,  74, 142,  55, 133,  21,\n",
              "         119,  87, 104, 113,  65, 115, 136,  39,  89, 137,  71, 133,  70,  29,\n",
              "          41, 109,  38, 109,  74, 131,  63,  55, 130,  71,  43,  21,  92, 107,\n",
              "           9, 123,  25]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long)"
      ],
      "metadata": {
        "id": "lGKBrbFZNm46"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits, loss = m.forward(idx)"
      ],
      "metadata": {
        "id": "4W3OYmcPNuEG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pOah6wkNwQb",
        "outputId": "cf8ac922-ef28-43df-a384-54958d7621db"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 147])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = logits[:, -1, :] # becomes (B, C)"
      ],
      "metadata": {
        "id": "A0_HCHsVNxdX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_doVRMKjN1XW",
        "outputId": "82f1b2f5-1088-4b20-d04a-b41282b3fe5b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 147])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs = F.softmax(logits, dim=-1) # (B, C)"
      ],
      "metadata": {
        "id": "jg1KTDRjN4OJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CAPMxW7N6tQ",
        "outputId": "c1248933-d82e-49b1-9fb5-8729932e499d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0043, 0.0033, 0.0025, 0.0014, 0.0067, 0.0037, 0.0093, 0.0038, 0.0051,\n",
              "         0.0115, 0.0009, 0.0022, 0.0045, 0.0028, 0.0014, 0.0168, 0.0138, 0.0031,\n",
              "         0.0048, 0.0094, 0.0005, 0.0059, 0.0159, 0.0065, 0.0041, 0.0008, 0.0011,\n",
              "         0.0026, 0.0056, 0.0016, 0.0165, 0.0441, 0.0018, 0.0028, 0.0099, 0.0041,\n",
              "         0.0042, 0.0112, 0.0011, 0.0027, 0.0022, 0.0014, 0.0062, 0.0008, 0.0011,\n",
              "         0.0064, 0.0020, 0.0018, 0.0186, 0.0016, 0.0139, 0.0027, 0.0008, 0.0294,\n",
              "         0.0569, 0.0006, 0.0153, 0.0008, 0.0082, 0.0029, 0.0078, 0.0166, 0.0179,\n",
              "         0.0024, 0.0016, 0.0065, 0.0034, 0.0034, 0.0022, 0.0057, 0.0028, 0.0012,\n",
              "         0.0267, 0.0021, 0.0045, 0.0072, 0.0009, 0.0089, 0.0041, 0.0045, 0.0433,\n",
              "         0.0011, 0.0099, 0.0063, 0.0007, 0.0010, 0.0072, 0.0029, 0.0051, 0.0025,\n",
              "         0.0058, 0.0017, 0.0024, 0.0023, 0.0013, 0.0024, 0.0007, 0.0009, 0.0030,\n",
              "         0.0094, 0.0037, 0.0017, 0.0027, 0.0036, 0.0030, 0.0031, 0.0048, 0.0143,\n",
              "         0.0040, 0.0456, 0.0019, 0.0005, 0.0026, 0.0036, 0.0091, 0.0005, 0.0042,\n",
              "         0.0057, 0.0006, 0.0064, 0.0207, 0.0048, 0.0049, 0.0019, 0.0019, 0.0049,\n",
              "         0.0021, 0.0008, 0.0005, 0.0223, 0.0132, 0.0023, 0.0138, 0.0195, 0.0032,\n",
              "         0.0007, 0.0035, 0.0039, 0.0103, 0.0071, 0.0049, 0.0017, 0.0075, 0.0087,\n",
              "         0.0353, 0.0094, 0.0008]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(probs).view([1,1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkYSMrPFN8mb",
        "outputId": "ae9e65b4-0c44-436e-ef9d-e23302d82a59"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[54]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)"
      ],
      "metadata": {
        "id": "oTLJ04MVN_PV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_next.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fseYpVtpOBU6",
        "outputId": "1c8041d5-63e9-4923-c1a1-fb75c0a99c98"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzYQekl0ODVn",
        "outputId": "11441bb5-0662-4201-e1f6-02af6a273090"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[30]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs[0,idx_next]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV3-aKr_OEMc",
        "outputId": "554932e0-951c-46c9-a975-b6955f0c1ad2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0165]], grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(probs,dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlQ5fMiHOITC",
        "outputId": "8a1dd379-0936-42a3-ed78-78678e6ac753"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([54])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the predictions\n",
        "logits, loss = m.forward(idx)\n",
        "# focus only on the last time step\n",
        "logits = logits[:, -1, :] # becomes (B, C)\n",
        "# apply softmax to get probabilities\n",
        "probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "# sample from the distribution\n",
        "idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "# append sampled index to the running sequence\n",
        "idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)"
      ],
      "metadata": {
        "id": "YjQvrly2OKZJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "LHJfsazAONb8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results... \n",
        "    \n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doKQTI7OOP0O",
        "outputId": "388ffe87-6f9c-47f0-a160-7b1de46145e9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.387226581573486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L310lxQ8ORwL",
        "outputId": "a0804cbf-f0e5-42cf-be0c-eed073062c03"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tãADúq%ËBèÂKUnnhôF$§3ËiêûMìpëâ(ub)x7zWÃ§ñmj4ZA:ñPx«EEçfW3Km½­óLkàâËûjËg§Ood½ ùëññÉ&¿)2cáÊcdY5ÀF:u(&½%o:ç¡UâùÃûzº;e¿ÓÚT`ÂÈ0»a-CäËëñA1?§¿sIeõKm*mö5.»ìWCÈl(È´ºàO­â`0ÛRÓÁ½ó?Nuz69ü?íü`àVàêA´8NB 87ÔYÓ-oRf4¡qJúUPb]ËuM'*2KñM6*K-qûOY2º'äâ=ç KÇ`;È] ÕyÂIcTü,ízADàQ6W26òTckMAS*/lÔ Ü\tÚ,äÚ'/p7X:´Õ$Aî°sop´%TFÃ=7;XoXKW´%»%Â W8yû4FY6ÇrC`ë¡qªAqmwlOovóñËS]\"á9½G\tùºäE¿!­âÚ2WÕmz[7\"áãÛY'oOû1­ägíÔrnÊnícÁcÔUwóPt.=N+ÕfLU&rGüÁüâóù_­FNdh¡sqm½zÔ1FK/Ú,%péPãY.´;KÜî0ûS¡­O¡;sÀA§\n",
            "h0éô_íz]§&%p'ËF(UkàOD_ñD$\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "  \n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "533iiNnzOXL5",
        "outputId": "9b5cca44-3392-4bfb-f751-bc3d98d2bd63"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 120000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "import nltk\n",
        "nltk.download('machado')\n",
        "from nltk.corpus import machado\n",
        "text=machado.raw()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "    def generate_final(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.argmax(probs).view([1,1])\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSSI6w3wOgpt",
        "outputId": "effb00d2-61c0-408c-f8af-f0a181b42889"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]   Package machado is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.220307 M parameters\n",
            "step 0: train loss 5.2092, val loss 5.2124\n",
            "step 100: train loss 2.6881, val loss 2.8650\n",
            "step 200: train loss 2.5287, val loss 2.6908\n",
            "step 300: train loss 2.4631, val loss 2.6037\n",
            "step 400: train loss 2.4085, val loss 2.5562\n",
            "step 500: train loss 2.3622, val loss 2.5132\n",
            "step 600: train loss 2.2941, val loss 2.4093\n",
            "step 700: train loss 2.2577, val loss 2.3856\n",
            "step 800: train loss 2.2295, val loss 2.3596\n",
            "step 900: train loss 2.1980, val loss 2.3245\n",
            "step 1000: train loss 2.1836, val loss 2.2906\n",
            "step 1100: train loss 2.1533, val loss 2.2548\n",
            "step 1200: train loss 2.1311, val loss 2.2709\n",
            "step 1300: train loss 2.1132, val loss 2.2251\n",
            "step 1400: train loss 2.0820, val loss 2.1964\n",
            "step 1500: train loss 2.0621, val loss 2.1777\n",
            "step 1600: train loss 2.0427, val loss 2.1612\n",
            "step 1700: train loss 2.0286, val loss 2.1643\n",
            "step 1800: train loss 2.0177, val loss 2.1460\n",
            "step 1900: train loss 2.0086, val loss 2.1315\n",
            "step 2000: train loss 2.0069, val loss 2.1302\n",
            "step 2100: train loss 1.9970, val loss 2.1166\n",
            "step 2200: train loss 1.9593, val loss 2.0874\n",
            "step 2300: train loss 1.9623, val loss 2.0828\n",
            "step 2400: train loss 1.9362, val loss 2.0848\n",
            "step 2500: train loss 1.9430, val loss 2.0868\n",
            "step 2600: train loss 1.9217, val loss 2.0542\n",
            "step 2700: train loss 1.9220, val loss 2.0661\n",
            "step 2800: train loss 1.9137, val loss 2.0618\n",
            "step 2900: train loss 1.8912, val loss 2.0482\n",
            "step 3000: train loss 1.8929, val loss 2.0290\n",
            "step 3100: train loss 1.8905, val loss 2.0365\n",
            "step 3200: train loss 1.8725, val loss 2.0225\n",
            "step 3300: train loss 1.8695, val loss 2.0165\n",
            "step 3400: train loss 1.8697, val loss 2.0097\n",
            "step 3500: train loss 1.8744, val loss 2.0146\n",
            "step 3600: train loss 1.8462, val loss 1.9957\n",
            "step 3700: train loss 1.8429, val loss 1.9946\n",
            "step 3800: train loss 1.8463, val loss 2.0096\n",
            "step 3900: train loss 1.8380, val loss 1.9821\n",
            "step 4000: train loss 1.8290, val loss 1.9731\n",
            "step 4100: train loss 1.8298, val loss 1.9748\n",
            "step 4200: train loss 1.8202, val loss 1.9801\n",
            "step 4300: train loss 1.8187, val loss 1.9644\n",
            "step 4400: train loss 1.8128, val loss 1.9716\n",
            "step 4500: train loss 1.8144, val loss 1.9650\n",
            "step 4600: train loss 1.7916, val loss 1.9550\n",
            "step 4700: train loss 1.8060, val loss 1.9566\n",
            "step 4800: train loss 1.8006, val loss 1.9592\n",
            "step 4900: train loss 1.7901, val loss 1.9513\n",
            "step 5000: train loss 1.8036, val loss 1.9500\n",
            "step 5100: train loss 1.7847, val loss 1.9389\n",
            "step 5200: train loss 1.7877, val loss 1.9470\n",
            "step 5300: train loss 1.7777, val loss 1.9302\n",
            "step 5400: train loss 1.7875, val loss 1.9389\n",
            "step 5500: train loss 1.7673, val loss 1.9280\n",
            "step 5600: train loss 1.7685, val loss 1.9500\n",
            "step 5700: train loss 1.7594, val loss 1.9230\n",
            "step 5800: train loss 1.7562, val loss 1.9355\n",
            "step 5900: train loss 1.7586, val loss 1.9260\n",
            "step 6000: train loss 1.7567, val loss 1.9238\n",
            "step 6100: train loss 1.7545, val loss 1.9342\n",
            "step 6200: train loss 1.7527, val loss 1.9207\n",
            "step 6300: train loss 1.7553, val loss 1.9051\n",
            "step 6400: train loss 1.7453, val loss 1.9178\n",
            "step 6500: train loss 1.7453, val loss 1.9177\n",
            "step 6600: train loss 1.7439, val loss 1.9122\n",
            "step 6700: train loss 1.7487, val loss 1.9016\n",
            "step 6800: train loss 1.7368, val loss 1.9170\n",
            "step 6900: train loss 1.7395, val loss 1.9151\n",
            "step 7000: train loss 1.7384, val loss 1.8986\n",
            "step 7100: train loss 1.7332, val loss 1.9036\n",
            "step 7200: train loss 1.7248, val loss 1.9067\n",
            "step 7300: train loss 1.7282, val loss 1.9070\n",
            "step 7400: train loss 1.7273, val loss 1.9092\n",
            "step 7500: train loss 1.7244, val loss 1.8939\n",
            "step 7600: train loss 1.7163, val loss 1.8911\n",
            "step 7700: train loss 1.7342, val loss 1.8990\n",
            "step 7800: train loss 1.7338, val loss 1.9124\n",
            "step 7900: train loss 1.7229, val loss 1.8909\n",
            "step 8000: train loss 1.7155, val loss 1.8924\n",
            "step 8100: train loss 1.7126, val loss 1.8765\n",
            "step 8200: train loss 1.7067, val loss 1.8938\n",
            "step 8300: train loss 1.7135, val loss 1.8744\n",
            "step 8400: train loss 1.7125, val loss 1.8915\n",
            "step 8500: train loss 1.7066, val loss 1.8744\n",
            "step 8600: train loss 1.7041, val loss 1.8869\n",
            "step 8700: train loss 1.6986, val loss 1.8700\n",
            "step 8800: train loss 1.6970, val loss 1.8821\n",
            "step 8900: train loss 1.7041, val loss 1.8942\n",
            "step 9000: train loss 1.6923, val loss 1.8658\n",
            "step 9100: train loss 1.6908, val loss 1.8776\n",
            "step 9200: train loss 1.6940, val loss 1.8807\n",
            "step 9300: train loss 1.6927, val loss 1.8853\n",
            "step 9400: train loss 1.6961, val loss 1.8793\n",
            "step 9500: train loss 1.7068, val loss 1.8780\n",
            "step 9600: train loss 1.6876, val loss 1.8647\n",
            "step 9700: train loss 1.6887, val loss 1.8620\n",
            "step 9800: train loss 1.6914, val loss 1.8584\n",
            "step 9900: train loss 1.6910, val loss 1.8639\n",
            "step 10000: train loss 1.6824, val loss 1.8582\n",
            "step 10100: train loss 1.6816, val loss 1.8562\n",
            "step 10200: train loss 1.6798, val loss 1.8554\n",
            "step 10300: train loss 1.6791, val loss 1.8455\n",
            "step 10400: train loss 1.6777, val loss 1.8496\n",
            "step 10500: train loss 1.6869, val loss 1.8597\n",
            "step 10600: train loss 1.6862, val loss 1.8620\n",
            "step 10700: train loss 1.6794, val loss 1.8558\n",
            "step 10800: train loss 1.6745, val loss 1.8520\n",
            "step 10900: train loss 1.6675, val loss 1.8518\n",
            "step 11000: train loss 1.6705, val loss 1.8646\n",
            "step 11100: train loss 1.6759, val loss 1.8542\n",
            "step 11200: train loss 1.6560, val loss 1.8379\n",
            "step 11300: train loss 1.6637, val loss 1.8324\n",
            "step 11400: train loss 1.6572, val loss 1.8550\n",
            "step 11500: train loss 1.6605, val loss 1.8361\n",
            "step 11600: train loss 1.6644, val loss 1.8339\n",
            "step 11700: train loss 1.6664, val loss 1.8475\n",
            "step 11800: train loss 1.6604, val loss 1.8416\n",
            "step 11900: train loss 1.6615, val loss 1.8408\n",
            "step 12000: train loss 1.6641, val loss 1.8518\n",
            "step 12100: train loss 1.6705, val loss 1.8469\n",
            "step 12200: train loss 1.6661, val loss 1.8297\n",
            "step 12300: train loss 1.6646, val loss 1.8515\n",
            "step 12400: train loss 1.6554, val loss 1.8501\n",
            "step 12500: train loss 1.6480, val loss 1.8485\n",
            "step 12600: train loss 1.6632, val loss 1.8404\n",
            "step 12700: train loss 1.6574, val loss 1.8372\n",
            "step 12800: train loss 1.6590, val loss 1.8391\n",
            "step 12900: train loss 1.6700, val loss 1.8474\n",
            "step 13000: train loss 1.6448, val loss 1.8269\n",
            "step 13100: train loss 1.6557, val loss 1.8460\n",
            "step 13200: train loss 1.6569, val loss 1.8314\n",
            "step 13300: train loss 1.6429, val loss 1.8333\n",
            "step 13400: train loss 1.6534, val loss 1.8216\n",
            "step 13500: train loss 1.6568, val loss 1.8428\n",
            "step 13600: train loss 1.6475, val loss 1.8350\n",
            "step 13700: train loss 1.6543, val loss 1.8368\n",
            "step 13800: train loss 1.6500, val loss 1.8303\n",
            "step 13900: train loss 1.6459, val loss 1.8269\n",
            "step 14000: train loss 1.6552, val loss 1.8258\n",
            "step 14100: train loss 1.6353, val loss 1.8367\n",
            "step 14200: train loss 1.6519, val loss 1.8254\n",
            "step 14300: train loss 1.6475, val loss 1.8344\n",
            "step 14400: train loss 1.6369, val loss 1.8302\n",
            "step 14500: train loss 1.6372, val loss 1.8291\n",
            "step 14600: train loss 1.6438, val loss 1.8307\n",
            "step 14700: train loss 1.6465, val loss 1.8187\n",
            "step 14800: train loss 1.6314, val loss 1.8242\n",
            "step 14900: train loss 1.6430, val loss 1.8140\n",
            "step 15000: train loss 1.6377, val loss 1.8244\n",
            "step 15100: train loss 1.6431, val loss 1.8281\n",
            "step 15200: train loss 1.6378, val loss 1.8236\n",
            "step 15300: train loss 1.6373, val loss 1.8242\n",
            "step 15400: train loss 1.6339, val loss 1.8221\n",
            "step 15500: train loss 1.6319, val loss 1.8288\n",
            "step 15600: train loss 1.6292, val loss 1.8216\n",
            "step 15700: train loss 1.6284, val loss 1.8217\n",
            "step 15800: train loss 1.6263, val loss 1.8207\n",
            "step 15900: train loss 1.6248, val loss 1.8101\n",
            "step 16000: train loss 1.6319, val loss 1.8305\n",
            "step 16100: train loss 1.6284, val loss 1.8158\n",
            "step 16200: train loss 1.6273, val loss 1.8059\n",
            "step 16300: train loss 1.6273, val loss 1.8036\n",
            "step 16400: train loss 1.6255, val loss 1.8116\n",
            "step 16500: train loss 1.6284, val loss 1.8241\n",
            "step 16600: train loss 1.6257, val loss 1.8165\n",
            "step 16700: train loss 1.6180, val loss 1.8088\n",
            "step 16800: train loss 1.6283, val loss 1.8128\n",
            "step 16900: train loss 1.6194, val loss 1.8108\n",
            "step 17000: train loss 1.6381, val loss 1.8133\n",
            "step 17100: train loss 1.6241, val loss 1.8271\n",
            "step 17200: train loss 1.6282, val loss 1.8121\n",
            "step 17300: train loss 1.6360, val loss 1.8079\n",
            "step 17400: train loss 1.6271, val loss 1.8237\n",
            "step 17500: train loss 1.6121, val loss 1.8032\n",
            "step 17600: train loss 1.6250, val loss 1.8065\n",
            "step 17700: train loss 1.6243, val loss 1.8078\n",
            "step 17800: train loss 1.6258, val loss 1.8122\n",
            "step 17900: train loss 1.6280, val loss 1.8048\n",
            "step 18000: train loss 1.6150, val loss 1.8179\n",
            "step 18100: train loss 1.6176, val loss 1.8227\n",
            "step 18200: train loss 1.6144, val loss 1.7951\n",
            "step 18300: train loss 1.6186, val loss 1.7942\n",
            "step 18400: train loss 1.6131, val loss 1.7930\n",
            "step 18500: train loss 1.6112, val loss 1.8061\n",
            "step 18600: train loss 1.6173, val loss 1.7984\n",
            "step 18700: train loss 1.6132, val loss 1.7936\n",
            "step 18800: train loss 1.6243, val loss 1.8158\n",
            "step 18900: train loss 1.6058, val loss 1.8032\n",
            "step 19000: train loss 1.6129, val loss 1.7970\n",
            "step 19100: train loss 1.6019, val loss 1.8040\n",
            "step 19200: train loss 1.6085, val loss 1.7931\n",
            "step 19300: train loss 1.6006, val loss 1.7947\n",
            "step 19400: train loss 1.6164, val loss 1.8077\n",
            "step 19500: train loss 1.6158, val loss 1.8118\n",
            "step 19600: train loss 1.6116, val loss 1.8091\n",
            "step 19700: train loss 1.6115, val loss 1.7929\n",
            "step 19800: train loss 1.6097, val loss 1.7967\n",
            "step 19900: train loss 1.6077, val loss 1.8087\n",
            "step 20000: train loss 1.6068, val loss 1.7879\n",
            "step 20100: train loss 1.6130, val loss 1.8038\n",
            "step 20200: train loss 1.6148, val loss 1.7988\n",
            "step 20300: train loss 1.6173, val loss 1.8213\n",
            "step 20400: train loss 1.6034, val loss 1.7989\n",
            "step 20500: train loss 1.6074, val loss 1.7819\n",
            "step 20600: train loss 1.6160, val loss 1.7979\n",
            "step 20700: train loss 1.6061, val loss 1.7988\n",
            "step 20800: train loss 1.6042, val loss 1.7961\n",
            "step 20900: train loss 1.6143, val loss 1.7966\n",
            "step 21000: train loss 1.6050, val loss 1.7923\n",
            "step 21100: train loss 1.5941, val loss 1.7894\n",
            "step 21200: train loss 1.6114, val loss 1.7953\n",
            "step 21300: train loss 1.6061, val loss 1.7831\n",
            "step 21400: train loss 1.6052, val loss 1.8089\n",
            "step 21500: train loss 1.6071, val loss 1.7823\n",
            "step 21600: train loss 1.6003, val loss 1.7995\n",
            "step 21700: train loss 1.5956, val loss 1.8011\n",
            "step 21800: train loss 1.5993, val loss 1.7861\n",
            "step 21900: train loss 1.5943, val loss 1.7848\n",
            "step 22000: train loss 1.5929, val loss 1.7797\n",
            "step 22100: train loss 1.5939, val loss 1.7975\n",
            "step 22200: train loss 1.5945, val loss 1.7849\n",
            "step 22300: train loss 1.5908, val loss 1.7899\n",
            "step 22400: train loss 1.5975, val loss 1.7850\n",
            "step 22500: train loss 1.5963, val loss 1.7777\n",
            "step 22600: train loss 1.5936, val loss 1.7882\n",
            "step 22700: train loss 1.5952, val loss 1.7747\n",
            "step 22800: train loss 1.5920, val loss 1.7804\n",
            "step 22900: train loss 1.5880, val loss 1.7682\n",
            "step 23000: train loss 1.5997, val loss 1.7960\n",
            "step 23100: train loss 1.5880, val loss 1.7916\n",
            "step 23200: train loss 1.5996, val loss 1.7978\n",
            "step 23300: train loss 1.5955, val loss 1.7792\n",
            "step 23400: train loss 1.5954, val loss 1.7795\n",
            "step 23500: train loss 1.6158, val loss 1.7956\n",
            "step 23600: train loss 1.5887, val loss 1.7749\n",
            "step 23700: train loss 1.5975, val loss 1.7823\n",
            "step 23800: train loss 1.5952, val loss 1.7758\n",
            "step 23900: train loss 1.5872, val loss 1.7736\n",
            "step 24000: train loss 1.5907, val loss 1.7843\n",
            "step 24100: train loss 1.5892, val loss 1.7806\n",
            "step 24200: train loss 1.5841, val loss 1.7785\n",
            "step 24300: train loss 1.5779, val loss 1.8028\n",
            "step 24400: train loss 1.5921, val loss 1.7671\n",
            "step 24500: train loss 1.5895, val loss 1.7738\n",
            "step 24600: train loss 1.5835, val loss 1.7712\n",
            "step 24700: train loss 1.5983, val loss 1.7855\n",
            "step 24800: train loss 1.5842, val loss 1.7775\n",
            "step 24900: train loss 1.5874, val loss 1.7839\n",
            "step 25000: train loss 1.5797, val loss 1.7785\n",
            "step 25100: train loss 1.5974, val loss 1.7963\n",
            "step 25200: train loss 1.5838, val loss 1.7944\n",
            "step 25300: train loss 1.5935, val loss 1.7761\n",
            "step 25400: train loss 1.5948, val loss 1.7746\n",
            "step 25500: train loss 1.5828, val loss 1.7739\n",
            "step 25600: train loss 1.5929, val loss 1.7689\n",
            "step 25700: train loss 1.5884, val loss 1.7621\n",
            "step 25800: train loss 1.5995, val loss 1.7797\n",
            "step 25900: train loss 1.5816, val loss 1.7844\n",
            "step 26000: train loss 1.5881, val loss 1.7793\n",
            "step 26100: train loss 1.5866, val loss 1.7757\n",
            "step 26200: train loss 1.5816, val loss 1.7770\n",
            "step 26300: train loss 1.5844, val loss 1.7703\n",
            "step 26400: train loss 1.5739, val loss 1.7828\n",
            "step 26500: train loss 1.5784, val loss 1.7651\n",
            "step 26600: train loss 1.5829, val loss 1.7819\n",
            "step 26700: train loss 1.5818, val loss 1.7814\n",
            "step 26800: train loss 1.5854, val loss 1.7639\n",
            "step 26900: train loss 1.5717, val loss 1.7737\n",
            "step 27000: train loss 1.5847, val loss 1.7895\n",
            "step 27100: train loss 1.5846, val loss 1.7833\n",
            "step 27200: train loss 1.5750, val loss 1.7744\n",
            "step 27300: train loss 1.5989, val loss 1.7738\n",
            "step 27400: train loss 1.5779, val loss 1.7890\n",
            "step 27500: train loss 1.5865, val loss 1.7689\n",
            "step 27600: train loss 1.5880, val loss 1.7715\n",
            "step 27700: train loss 1.5810, val loss 1.7581\n",
            "step 27800: train loss 1.5917, val loss 1.7641\n",
            "step 27900: train loss 1.5715, val loss 1.7775\n",
            "step 28000: train loss 1.5799, val loss 1.7558\n",
            "step 28100: train loss 1.5813, val loss 1.7628\n",
            "step 28200: train loss 1.5803, val loss 1.7680\n",
            "step 28300: train loss 1.5775, val loss 1.7825\n",
            "step 28400: train loss 1.5772, val loss 1.7668\n",
            "step 28500: train loss 1.5785, val loss 1.7668\n",
            "step 28600: train loss 1.5756, val loss 1.7557\n",
            "step 28700: train loss 1.5826, val loss 1.7813\n",
            "step 28800: train loss 1.5738, val loss 1.7652\n",
            "step 28900: train loss 1.5761, val loss 1.7642\n",
            "step 29000: train loss 1.5733, val loss 1.7763\n",
            "step 29100: train loss 1.5700, val loss 1.7787\n",
            "step 29200: train loss 1.5796, val loss 1.7721\n",
            "step 29300: train loss 1.5785, val loss 1.7683\n",
            "step 29400: train loss 1.5684, val loss 1.7820\n",
            "step 29500: train loss 1.5816, val loss 1.7771\n",
            "step 29600: train loss 1.5697, val loss 1.7695\n",
            "step 29700: train loss 1.5696, val loss 1.7565\n",
            "step 29800: train loss 1.5779, val loss 1.7671\n",
            "step 29900: train loss 1.5715, val loss 1.7625\n",
            "step 30000: train loss 1.5560, val loss 1.7626\n",
            "step 30100: train loss 1.5678, val loss 1.7737\n",
            "step 30200: train loss 1.5697, val loss 1.7687\n",
            "step 30300: train loss 1.5782, val loss 1.7653\n",
            "step 30400: train loss 1.5693, val loss 1.7768\n",
            "step 30500: train loss 1.5700, val loss 1.7611\n",
            "step 30600: train loss 1.5609, val loss 1.7501\n",
            "step 30700: train loss 1.5669, val loss 1.7770\n",
            "step 30800: train loss 1.5683, val loss 1.7620\n",
            "step 30900: train loss 1.5816, val loss 1.7714\n",
            "step 31000: train loss 1.5737, val loss 1.7713\n",
            "step 31100: train loss 1.5758, val loss 1.7645\n",
            "step 31200: train loss 1.5619, val loss 1.7633\n",
            "step 31300: train loss 1.5697, val loss 1.7614\n",
            "step 31400: train loss 1.5577, val loss 1.7579\n",
            "step 31500: train loss 1.5692, val loss 1.7675\n",
            "step 31600: train loss 1.5671, val loss 1.7754\n",
            "step 31700: train loss 1.5784, val loss 1.7473\n",
            "step 31800: train loss 1.5697, val loss 1.7432\n",
            "step 31900: train loss 1.5638, val loss 1.7563\n",
            "step 32000: train loss 1.5712, val loss 1.7503\n",
            "step 32100: train loss 1.5752, val loss 1.7610\n",
            "step 32200: train loss 1.5723, val loss 1.7646\n",
            "step 32300: train loss 1.5717, val loss 1.7585\n",
            "step 32400: train loss 1.5545, val loss 1.7616\n",
            "step 32500: train loss 1.5717, val loss 1.7720\n",
            "step 32600: train loss 1.5651, val loss 1.7573\n",
            "step 32700: train loss 1.5632, val loss 1.7586\n",
            "step 32800: train loss 1.5703, val loss 1.7536\n",
            "step 32900: train loss 1.5714, val loss 1.7629\n",
            "step 33000: train loss 1.5631, val loss 1.7581\n",
            "step 33100: train loss 1.5549, val loss 1.7600\n",
            "step 33200: train loss 1.5660, val loss 1.7583\n",
            "step 33300: train loss 1.5609, val loss 1.7575\n",
            "step 33400: train loss 1.5729, val loss 1.7562\n",
            "step 33500: train loss 1.5536, val loss 1.7566\n",
            "step 33600: train loss 1.5669, val loss 1.7773\n",
            "step 33700: train loss 1.5701, val loss 1.7477\n",
            "step 33800: train loss 1.5659, val loss 1.7644\n",
            "step 33900: train loss 1.5527, val loss 1.7493\n",
            "step 34000: train loss 1.5576, val loss 1.7590\n",
            "step 34100: train loss 1.5565, val loss 1.7589\n",
            "step 34200: train loss 1.5670, val loss 1.7600\n",
            "step 34300: train loss 1.5637, val loss 1.7607\n",
            "step 34400: train loss 1.5560, val loss 1.7667\n",
            "step 34500: train loss 1.5661, val loss 1.7376\n",
            "step 34600: train loss 1.5795, val loss 1.7467\n",
            "step 34700: train loss 1.5591, val loss 1.7758\n",
            "step 34800: train loss 1.5649, val loss 1.7602\n",
            "step 34900: train loss 1.5598, val loss 1.7516\n",
            "step 35000: train loss 1.5759, val loss 1.7623\n",
            "step 35100: train loss 1.5724, val loss 1.7572\n",
            "step 35200: train loss 1.5691, val loss 1.7667\n",
            "step 35300: train loss 1.5675, val loss 1.7576\n",
            "step 35400: train loss 1.5632, val loss 1.7672\n",
            "step 35500: train loss 1.5506, val loss 1.7521\n",
            "step 35600: train loss 1.5626, val loss 1.7596\n",
            "step 35700: train loss 1.5520, val loss 1.7477\n",
            "step 35800: train loss 1.5572, val loss 1.7582\n",
            "step 35900: train loss 1.5665, val loss 1.7500\n",
            "step 36000: train loss 1.5521, val loss 1.7669\n",
            "step 36100: train loss 1.5529, val loss 1.7574\n",
            "step 36200: train loss 1.5597, val loss 1.7547\n",
            "step 36300: train loss 1.5679, val loss 1.7553\n",
            "step 36400: train loss 1.5632, val loss 1.7572\n",
            "step 36500: train loss 1.5596, val loss 1.7515\n",
            "step 36600: train loss 1.5611, val loss 1.7482\n",
            "step 36700: train loss 1.5604, val loss 1.7561\n",
            "step 36800: train loss 1.5618, val loss 1.7643\n",
            "step 36900: train loss 1.5471, val loss 1.7467\n",
            "step 37000: train loss 1.5581, val loss 1.7485\n",
            "step 37100: train loss 1.5586, val loss 1.7535\n",
            "step 37200: train loss 1.5580, val loss 1.7535\n",
            "step 37300: train loss 1.5479, val loss 1.7276\n",
            "step 37400: train loss 1.5654, val loss 1.7529\n",
            "step 37500: train loss 1.5495, val loss 1.7555\n",
            "step 37600: train loss 1.5546, val loss 1.7530\n",
            "step 37700: train loss 1.5578, val loss 1.7532\n",
            "step 37800: train loss 1.5503, val loss 1.7510\n",
            "step 37900: train loss 1.5604, val loss 1.7468\n",
            "step 38000: train loss 1.5509, val loss 1.7615\n",
            "step 38100: train loss 1.5515, val loss 1.7625\n",
            "step 38200: train loss 1.5636, val loss 1.7579\n",
            "step 38300: train loss 1.5463, val loss 1.7493\n",
            "step 38400: train loss 1.5634, val loss 1.7514\n",
            "step 38500: train loss 1.5586, val loss 1.7412\n",
            "step 38600: train loss 1.5459, val loss 1.7471\n",
            "step 38700: train loss 1.5594, val loss 1.7454\n",
            "step 38800: train loss 1.5606, val loss 1.7496\n",
            "step 38900: train loss 1.5570, val loss 1.7616\n",
            "step 39000: train loss 1.5582, val loss 1.7418\n",
            "step 39100: train loss 1.5640, val loss 1.7540\n",
            "step 39200: train loss 1.5486, val loss 1.7482\n",
            "step 39300: train loss 1.5534, val loss 1.7527\n",
            "step 39400: train loss 1.5529, val loss 1.7553\n",
            "step 39500: train loss 1.5484, val loss 1.7389\n",
            "step 39600: train loss 1.5563, val loss 1.7609\n",
            "step 39700: train loss 1.5554, val loss 1.7617\n",
            "step 39800: train loss 1.5491, val loss 1.7543\n",
            "step 39900: train loss 1.5461, val loss 1.7506\n",
            "step 40000: train loss 1.5469, val loss 1.7463\n",
            "step 40100: train loss 1.5539, val loss 1.7545\n",
            "step 40200: train loss 1.5455, val loss 1.7565\n",
            "step 40300: train loss 1.5633, val loss 1.7455\n",
            "step 40400: train loss 1.5658, val loss 1.7348\n",
            "step 40500: train loss 1.5565, val loss 1.7612\n",
            "step 40600: train loss 1.5519, val loss 1.7417\n",
            "step 40700: train loss 1.5560, val loss 1.7376\n",
            "step 40800: train loss 1.5577, val loss 1.7531\n",
            "step 40900: train loss 1.5577, val loss 1.7481\n",
            "step 41000: train loss 1.5532, val loss 1.7348\n",
            "step 41100: train loss 1.5521, val loss 1.7469\n",
            "step 41200: train loss 1.5541, val loss 1.7430\n",
            "step 41300: train loss 1.5555, val loss 1.7484\n",
            "step 41400: train loss 1.5575, val loss 1.7462\n",
            "step 41500: train loss 1.5418, val loss 1.7588\n",
            "step 41600: train loss 1.5519, val loss 1.7312\n",
            "step 41700: train loss 1.5488, val loss 1.7533\n",
            "step 41800: train loss 1.5541, val loss 1.7548\n",
            "step 41900: train loss 1.5550, val loss 1.7413\n",
            "step 42000: train loss 1.5491, val loss 1.7429\n",
            "step 42100: train loss 1.5443, val loss 1.7327\n",
            "step 42200: train loss 1.5469, val loss 1.7517\n",
            "step 42300: train loss 1.5609, val loss 1.7443\n",
            "step 42400: train loss 1.5530, val loss 1.7420\n",
            "step 42500: train loss 1.5492, val loss 1.7389\n",
            "step 42600: train loss 1.5461, val loss 1.7538\n",
            "step 42700: train loss 1.5472, val loss 1.7411\n",
            "step 42800: train loss 1.5497, val loss 1.7303\n",
            "step 42900: train loss 1.5473, val loss 1.7424\n",
            "step 43000: train loss 1.5403, val loss 1.7378\n",
            "step 43100: train loss 1.5423, val loss 1.7271\n",
            "step 43200: train loss 1.5508, val loss 1.7486\n",
            "step 43300: train loss 1.5426, val loss 1.7386\n",
            "step 43400: train loss 1.5480, val loss 1.7396\n",
            "step 43500: train loss 1.5474, val loss 1.7336\n",
            "step 43600: train loss 1.5547, val loss 1.7394\n",
            "step 43700: train loss 1.5495, val loss 1.7520\n",
            "step 43800: train loss 1.5415, val loss 1.7321\n",
            "step 43900: train loss 1.5503, val loss 1.7420\n",
            "step 44000: train loss 1.5448, val loss 1.7427\n",
            "step 44100: train loss 1.5470, val loss 1.7375\n",
            "step 44200: train loss 1.5456, val loss 1.7309\n",
            "step 44300: train loss 1.5440, val loss 1.7341\n",
            "step 44400: train loss 1.5400, val loss 1.7549\n",
            "step 44500: train loss 1.5411, val loss 1.7277\n",
            "step 44600: train loss 1.5499, val loss 1.7391\n",
            "step 44700: train loss 1.5461, val loss 1.7472\n",
            "step 44800: train loss 1.5456, val loss 1.7387\n",
            "step 44900: train loss 1.5460, val loss 1.7345\n",
            "step 45000: train loss 1.5517, val loss 1.7583\n",
            "step 45100: train loss 1.5407, val loss 1.7186\n",
            "step 45200: train loss 1.5532, val loss 1.7384\n",
            "step 45300: train loss 1.5364, val loss 1.7260\n",
            "step 45400: train loss 1.5456, val loss 1.7339\n",
            "step 45500: train loss 1.5427, val loss 1.7338\n",
            "step 45600: train loss 1.5395, val loss 1.7555\n",
            "step 45700: train loss 1.5398, val loss 1.7409\n",
            "step 45800: train loss 1.5384, val loss 1.7478\n",
            "step 45900: train loss 1.5539, val loss 1.7292\n",
            "step 46000: train loss 1.5451, val loss 1.7392\n",
            "step 46100: train loss 1.5510, val loss 1.7294\n",
            "step 46200: train loss 1.5398, val loss 1.7265\n",
            "step 46300: train loss 1.5476, val loss 1.7494\n",
            "step 46400: train loss 1.5438, val loss 1.7362\n",
            "step 46500: train loss 1.5338, val loss 1.7431\n",
            "step 46600: train loss 1.5424, val loss 1.7335\n",
            "step 46700: train loss 1.5345, val loss 1.7444\n",
            "step 46800: train loss 1.5346, val loss 1.7325\n",
            "step 46900: train loss 1.5414, val loss 1.7543\n",
            "step 47000: train loss 1.5365, val loss 1.7421\n",
            "step 47100: train loss 1.5407, val loss 1.7469\n",
            "step 47200: train loss 1.5479, val loss 1.7250\n",
            "step 47300: train loss 1.5365, val loss 1.7376\n",
            "step 47400: train loss 1.5358, val loss 1.7453\n",
            "step 47500: train loss 1.5401, val loss 1.7398\n",
            "step 47600: train loss 1.5370, val loss 1.7298\n",
            "step 47700: train loss 1.5482, val loss 1.7397\n",
            "step 47800: train loss 1.5342, val loss 1.7336\n",
            "step 47900: train loss 1.5405, val loss 1.7402\n",
            "step 48000: train loss 1.5316, val loss 1.7452\n",
            "step 48100: train loss 1.5331, val loss 1.7347\n",
            "step 48200: train loss 1.5521, val loss 1.7301\n",
            "step 48300: train loss 1.5447, val loss 1.7317\n",
            "step 48400: train loss 1.5372, val loss 1.7357\n",
            "step 48500: train loss 1.5470, val loss 1.7418\n",
            "step 48600: train loss 1.5463, val loss 1.7599\n",
            "step 48700: train loss 1.5415, val loss 1.7379\n",
            "step 48800: train loss 1.5422, val loss 1.7348\n",
            "step 48900: train loss 1.5474, val loss 1.7300\n",
            "step 49000: train loss 1.5453, val loss 1.7412\n",
            "step 49100: train loss 1.5405, val loss 1.7310\n",
            "step 49200: train loss 1.5355, val loss 1.7314\n",
            "step 49300: train loss 1.5404, val loss 1.7202\n",
            "step 49400: train loss 1.5290, val loss 1.7363\n",
            "step 49500: train loss 1.5445, val loss 1.7403\n",
            "step 49600: train loss 1.5316, val loss 1.7324\n",
            "step 49700: train loss 1.5396, val loss 1.7294\n",
            "step 49800: train loss 1.5454, val loss 1.7442\n",
            "step 49900: train loss 1.5544, val loss 1.7243\n",
            "step 50000: train loss 1.5321, val loss 1.7410\n",
            "step 50100: train loss 1.5430, val loss 1.7220\n",
            "step 50200: train loss 1.5503, val loss 1.7356\n",
            "step 50300: train loss 1.5357, val loss 1.7212\n",
            "step 50400: train loss 1.5510, val loss 1.7363\n",
            "step 50500: train loss 1.5448, val loss 1.7392\n",
            "step 50600: train loss 1.5231, val loss 1.7329\n",
            "step 50700: train loss 1.5437, val loss 1.7385\n",
            "step 50800: train loss 1.5297, val loss 1.7266\n",
            "step 50900: train loss 1.5389, val loss 1.7286\n",
            "step 51000: train loss 1.5384, val loss 1.7377\n",
            "step 51100: train loss 1.5332, val loss 1.7344\n",
            "step 51200: train loss 1.5399, val loss 1.7440\n",
            "step 51300: train loss 1.5372, val loss 1.7339\n",
            "step 51400: train loss 1.5306, val loss 1.7359\n",
            "step 51500: train loss 1.5413, val loss 1.7314\n",
            "step 51600: train loss 1.5388, val loss 1.7210\n",
            "step 51700: train loss 1.5302, val loss 1.7304\n",
            "step 51800: train loss 1.5405, val loss 1.7499\n",
            "step 51900: train loss 1.5383, val loss 1.7408\n",
            "step 52000: train loss 1.5354, val loss 1.7414\n",
            "step 52100: train loss 1.5322, val loss 1.7338\n",
            "step 52200: train loss 1.5493, val loss 1.7360\n",
            "step 52300: train loss 1.5371, val loss 1.7380\n",
            "step 52400: train loss 1.5284, val loss 1.7348\n",
            "step 52500: train loss 1.5372, val loss 1.7426\n",
            "step 52600: train loss 1.5423, val loss 1.7284\n",
            "step 52700: train loss 1.5479, val loss 1.7346\n",
            "step 52800: train loss 1.5411, val loss 1.7378\n",
            "step 52900: train loss 1.5366, val loss 1.7430\n",
            "step 53000: train loss 1.5411, val loss 1.7384\n",
            "step 53100: train loss 1.5363, val loss 1.7373\n",
            "step 53200: train loss 1.5382, val loss 1.7236\n",
            "step 53300: train loss 1.5294, val loss 1.7294\n",
            "step 53400: train loss 1.5402, val loss 1.7178\n",
            "step 53500: train loss 1.5444, val loss 1.7348\n",
            "step 53600: train loss 1.5393, val loss 1.7212\n",
            "step 53700: train loss 1.5377, val loss 1.7407\n",
            "step 53800: train loss 1.5381, val loss 1.7403\n",
            "step 53900: train loss 1.5395, val loss 1.7334\n",
            "step 54000: train loss 1.5236, val loss 1.7446\n",
            "step 54100: train loss 1.5394, val loss 1.7354\n",
            "step 54200: train loss 1.5392, val loss 1.7324\n",
            "step 54300: train loss 1.5487, val loss 1.7271\n",
            "step 54400: train loss 1.5281, val loss 1.7401\n",
            "step 54500: train loss 1.5341, val loss 1.7267\n",
            "step 54600: train loss 1.5271, val loss 1.7196\n",
            "step 54700: train loss 1.5335, val loss 1.7293\n",
            "step 54800: train loss 1.5390, val loss 1.7185\n",
            "step 54900: train loss 1.5322, val loss 1.7442\n",
            "step 55000: train loss 1.5435, val loss 1.7298\n",
            "step 55100: train loss 1.5308, val loss 1.7265\n",
            "step 55200: train loss 1.5284, val loss 1.7176\n",
            "step 55300: train loss 1.5320, val loss 1.7148\n",
            "step 55400: train loss 1.5363, val loss 1.7300\n",
            "step 55500: train loss 1.5373, val loss 1.7439\n",
            "step 55600: train loss 1.5331, val loss 1.7205\n",
            "step 55700: train loss 1.5364, val loss 1.7274\n",
            "step 55800: train loss 1.5351, val loss 1.7336\n",
            "step 55900: train loss 1.5342, val loss 1.7225\n",
            "step 56000: train loss 1.5274, val loss 1.7291\n",
            "step 56100: train loss 1.5337, val loss 1.7253\n",
            "step 56200: train loss 1.5323, val loss 1.7231\n",
            "step 56300: train loss 1.5255, val loss 1.7347\n",
            "step 56400: train loss 1.5279, val loss 1.7157\n",
            "step 56500: train loss 1.5345, val loss 1.7270\n",
            "step 56600: train loss 1.5379, val loss 1.7278\n",
            "step 56700: train loss 1.5442, val loss 1.7343\n",
            "step 56800: train loss 1.5373, val loss 1.7551\n",
            "step 56900: train loss 1.5366, val loss 1.7255\n",
            "step 57000: train loss 1.5288, val loss 1.7410\n",
            "step 57100: train loss 1.5346, val loss 1.7419\n",
            "step 57200: train loss 1.5335, val loss 1.7188\n",
            "step 57300: train loss 1.5313, val loss 1.7314\n",
            "step 57400: train loss 1.5278, val loss 1.7204\n",
            "step 57500: train loss 1.5319, val loss 1.7432\n",
            "step 57600: train loss 1.5377, val loss 1.7358\n",
            "step 57700: train loss 1.5257, val loss 1.7191\n",
            "step 57800: train loss 1.5293, val loss 1.7269\n",
            "step 57900: train loss 1.5351, val loss 1.7130\n",
            "step 58000: train loss 1.5243, val loss 1.7361\n",
            "step 58100: train loss 1.5351, val loss 1.7340\n",
            "step 58200: train loss 1.5352, val loss 1.7459\n",
            "step 58300: train loss 1.5344, val loss 1.7254\n",
            "step 58400: train loss 1.5343, val loss 1.7245\n",
            "step 58500: train loss 1.5333, val loss 1.7329\n",
            "step 58600: train loss 1.5347, val loss 1.7168\n",
            "step 58700: train loss 1.5332, val loss 1.7406\n",
            "step 58800: train loss 1.5340, val loss 1.7283\n",
            "step 58900: train loss 1.5242, val loss 1.7270\n",
            "step 59000: train loss 1.5292, val loss 1.7282\n",
            "step 59100: train loss 1.5372, val loss 1.7428\n",
            "step 59200: train loss 1.5167, val loss 1.7306\n",
            "step 59300: train loss 1.5362, val loss 1.7324\n",
            "step 59400: train loss 1.5294, val loss 1.7351\n",
            "step 59500: train loss 1.5322, val loss 1.7127\n",
            "step 59600: train loss 1.5237, val loss 1.7210\n",
            "step 59700: train loss 1.5407, val loss 1.7271\n",
            "step 59800: train loss 1.5274, val loss 1.7232\n",
            "step 59900: train loss 1.5294, val loss 1.7249\n",
            "step 60000: train loss 1.5267, val loss 1.7366\n",
            "step 60100: train loss 1.5280, val loss 1.7265\n",
            "step 60200: train loss 1.5315, val loss 1.7254\n",
            "step 60300: train loss 1.5335, val loss 1.7337\n",
            "step 60400: train loss 1.5295, val loss 1.7229\n",
            "step 60500: train loss 1.5323, val loss 1.7324\n",
            "step 60600: train loss 1.5243, val loss 1.7309\n",
            "step 60700: train loss 1.5271, val loss 1.7312\n",
            "step 60800: train loss 1.5255, val loss 1.7299\n",
            "step 60900: train loss 1.5226, val loss 1.7368\n",
            "step 61000: train loss 1.5234, val loss 1.7146\n",
            "step 61100: train loss 1.5371, val loss 1.7193\n",
            "step 61200: train loss 1.5363, val loss 1.7324\n",
            "step 61300: train loss 1.5298, val loss 1.7262\n",
            "step 61400: train loss 1.5302, val loss 1.7382\n",
            "step 61500: train loss 1.5242, val loss 1.7168\n",
            "step 61600: train loss 1.5348, val loss 1.7227\n",
            "step 61700: train loss 1.5288, val loss 1.7282\n",
            "step 61800: train loss 1.5252, val loss 1.7188\n",
            "step 61900: train loss 1.5196, val loss 1.7279\n",
            "step 62000: train loss 1.5232, val loss 1.7256\n",
            "step 62100: train loss 1.5182, val loss 1.7220\n",
            "step 62200: train loss 1.5185, val loss 1.7305\n",
            "step 62300: train loss 1.5209, val loss 1.7166\n",
            "step 62400: train loss 1.5239, val loss 1.7283\n",
            "step 62500: train loss 1.5308, val loss 1.7154\n",
            "step 62600: train loss 1.5273, val loss 1.7253\n",
            "step 62700: train loss 1.5244, val loss 1.7280\n",
            "step 62800: train loss 1.5286, val loss 1.7121\n",
            "step 62900: train loss 1.5198, val loss 1.7338\n",
            "step 63000: train loss 1.5246, val loss 1.7246\n",
            "step 63100: train loss 1.5354, val loss 1.7407\n",
            "step 63200: train loss 1.5180, val loss 1.7309\n",
            "step 63300: train loss 1.5267, val loss 1.7271\n",
            "step 63400: train loss 1.5225, val loss 1.7284\n",
            "step 63500: train loss 1.5201, val loss 1.7210\n",
            "step 63600: train loss 1.5298, val loss 1.7305\n",
            "step 63700: train loss 1.5289, val loss 1.7153\n",
            "step 63800: train loss 1.5217, val loss 1.7253\n",
            "step 63900: train loss 1.5173, val loss 1.7253\n",
            "step 64000: train loss 1.5338, val loss 1.7226\n",
            "step 64100: train loss 1.5198, val loss 1.7295\n",
            "step 64200: train loss 1.5219, val loss 1.7139\n",
            "step 64300: train loss 1.5205, val loss 1.7150\n",
            "step 64400: train loss 1.5267, val loss 1.7099\n",
            "step 64500: train loss 1.5236, val loss 1.7151\n",
            "step 64600: train loss 1.5282, val loss 1.7253\n",
            "step 64700: train loss 1.5228, val loss 1.7184\n",
            "step 64800: train loss 1.5271, val loss 1.7292\n",
            "step 64900: train loss 1.5302, val loss 1.7356\n",
            "step 65000: train loss 1.5187, val loss 1.7211\n",
            "step 65100: train loss 1.5196, val loss 1.7290\n",
            "step 65200: train loss 1.5223, val loss 1.7252\n",
            "step 65300: train loss 1.5249, val loss 1.7309\n",
            "step 65400: train loss 1.5249, val loss 1.7222\n",
            "step 65500: train loss 1.5171, val loss 1.7328\n",
            "step 65600: train loss 1.5257, val loss 1.7276\n",
            "step 65700: train loss 1.5220, val loss 1.7276\n",
            "step 65800: train loss 1.5188, val loss 1.7173\n",
            "step 65900: train loss 1.5259, val loss 1.7276\n",
            "step 66000: train loss 1.5207, val loss 1.7351\n",
            "step 66100: train loss 1.5288, val loss 1.7158\n",
            "step 66200: train loss 1.5326, val loss 1.7341\n",
            "step 66300: train loss 1.5016, val loss 1.7179\n",
            "step 66400: train loss 1.5237, val loss 1.7110\n",
            "step 66500: train loss 1.5252, val loss 1.7229\n",
            "step 66600: train loss 1.5195, val loss 1.7233\n",
            "step 66700: train loss 1.5193, val loss 1.7326\n",
            "step 66800: train loss 1.5238, val loss 1.7286\n",
            "step 66900: train loss 1.5120, val loss 1.7202\n",
            "step 67000: train loss 1.5161, val loss 1.7329\n",
            "step 67100: train loss 1.5141, val loss 1.7236\n",
            "step 67200: train loss 1.5225, val loss 1.7354\n",
            "step 67300: train loss 1.5280, val loss 1.7330\n",
            "step 67400: train loss 1.5241, val loss 1.7384\n",
            "step 67500: train loss 1.5276, val loss 1.7207\n",
            "step 67600: train loss 1.5280, val loss 1.7144\n",
            "step 67700: train loss 1.5190, val loss 1.7259\n",
            "step 67800: train loss 1.5197, val loss 1.7292\n",
            "step 67900: train loss 1.5151, val loss 1.7420\n",
            "step 68000: train loss 1.5231, val loss 1.7293\n",
            "step 68100: train loss 1.5257, val loss 1.7276\n",
            "step 68200: train loss 1.5265, val loss 1.7300\n",
            "step 68300: train loss 1.5198, val loss 1.7126\n",
            "step 68400: train loss 1.5225, val loss 1.7125\n",
            "step 68500: train loss 1.5250, val loss 1.7159\n",
            "step 68600: train loss 1.5193, val loss 1.7160\n",
            "step 68700: train loss 1.5167, val loss 1.7266\n",
            "step 68800: train loss 1.5146, val loss 1.7191\n",
            "step 68900: train loss 1.5322, val loss 1.7296\n",
            "step 69000: train loss 1.5178, val loss 1.7237\n",
            "step 69100: train loss 1.5225, val loss 1.7329\n",
            "step 69200: train loss 1.5179, val loss 1.7233\n",
            "step 69300: train loss 1.5118, val loss 1.7248\n",
            "step 69400: train loss 1.5176, val loss 1.7194\n",
            "step 69500: train loss 1.5250, val loss 1.7257\n",
            "step 69600: train loss 1.5286, val loss 1.7356\n",
            "step 69700: train loss 1.5203, val loss 1.7291\n",
            "step 69800: train loss 1.5266, val loss 1.7139\n",
            "step 69900: train loss 1.5125, val loss 1.7119\n",
            "step 70000: train loss 1.5286, val loss 1.7236\n",
            "step 70100: train loss 1.5105, val loss 1.7252\n",
            "step 70200: train loss 1.5048, val loss 1.7198\n",
            "step 70300: train loss 1.5243, val loss 1.7179\n",
            "step 70400: train loss 1.5250, val loss 1.7199\n",
            "step 70500: train loss 1.5167, val loss 1.7217\n",
            "step 70600: train loss 1.5147, val loss 1.7307\n",
            "step 70700: train loss 1.5187, val loss 1.7248\n",
            "step 70800: train loss 1.5225, val loss 1.7280\n",
            "step 70900: train loss 1.5234, val loss 1.7249\n",
            "step 71000: train loss 1.5203, val loss 1.7258\n",
            "step 71100: train loss 1.5302, val loss 1.7202\n",
            "step 71200: train loss 1.5132, val loss 1.7320\n",
            "step 71300: train loss 1.5172, val loss 1.7096\n",
            "step 71400: train loss 1.5118, val loss 1.7087\n",
            "step 71500: train loss 1.5212, val loss 1.7316\n",
            "step 71600: train loss 1.5182, val loss 1.7160\n",
            "step 71700: train loss 1.5205, val loss 1.7190\n",
            "step 71800: train loss 1.5235, val loss 1.7257\n",
            "step 71900: train loss 1.5160, val loss 1.7195\n",
            "step 72000: train loss 1.5240, val loss 1.7254\n",
            "step 72100: train loss 1.5228, val loss 1.7160\n",
            "step 72200: train loss 1.5177, val loss 1.7191\n",
            "step 72300: train loss 1.5232, val loss 1.7325\n",
            "step 72400: train loss 1.5207, val loss 1.7300\n",
            "step 72500: train loss 1.5226, val loss 1.7290\n",
            "step 72600: train loss 1.5172, val loss 1.7140\n",
            "step 72700: train loss 1.5215, val loss 1.7216\n",
            "step 72800: train loss 1.5139, val loss 1.7167\n",
            "step 72900: train loss 1.5246, val loss 1.7145\n",
            "step 73000: train loss 1.5221, val loss 1.7237\n",
            "step 73100: train loss 1.5213, val loss 1.7149\n",
            "step 73200: train loss 1.5260, val loss 1.7144\n",
            "step 73300: train loss 1.5158, val loss 1.7230\n",
            "step 73400: train loss 1.5125, val loss 1.7218\n",
            "step 73500: train loss 1.5123, val loss 1.7266\n",
            "step 73600: train loss 1.5203, val loss 1.7316\n",
            "step 73700: train loss 1.5083, val loss 1.7131\n",
            "step 73800: train loss 1.5181, val loss 1.7194\n",
            "step 73900: train loss 1.5243, val loss 1.7127\n",
            "step 74000: train loss 1.5133, val loss 1.7070\n",
            "step 74100: train loss 1.5145, val loss 1.7117\n",
            "step 74200: train loss 1.5159, val loss 1.7149\n",
            "step 74300: train loss 1.5160, val loss 1.7187\n",
            "step 74400: train loss 1.5143, val loss 1.7233\n",
            "step 74500: train loss 1.5212, val loss 1.7390\n",
            "step 74600: train loss 1.5261, val loss 1.7252\n",
            "step 74700: train loss 1.5103, val loss 1.7092\n",
            "step 74800: train loss 1.5183, val loss 1.7152\n",
            "step 74900: train loss 1.5243, val loss 1.7118\n",
            "step 75000: train loss 1.5086, val loss 1.7205\n",
            "step 75100: train loss 1.5187, val loss 1.7157\n",
            "step 75200: train loss 1.5350, val loss 1.7074\n",
            "step 75300: train loss 1.5205, val loss 1.7245\n",
            "step 75400: train loss 1.5093, val loss 1.7148\n",
            "step 75500: train loss 1.5081, val loss 1.7236\n",
            "step 75600: train loss 1.5153, val loss 1.7205\n",
            "step 75700: train loss 1.5175, val loss 1.7203\n",
            "step 75800: train loss 1.5135, val loss 1.7142\n",
            "step 75900: train loss 1.5167, val loss 1.7352\n",
            "step 76000: train loss 1.5242, val loss 1.7279\n",
            "step 76100: train loss 1.5114, val loss 1.7170\n",
            "step 76200: train loss 1.5122, val loss 1.7260\n",
            "step 76300: train loss 1.5140, val loss 1.7316\n",
            "step 76400: train loss 1.5200, val loss 1.7181\n",
            "step 76500: train loss 1.5114, val loss 1.7097\n",
            "step 76600: train loss 1.5204, val loss 1.7169\n",
            "step 76700: train loss 1.5166, val loss 1.7238\n",
            "step 76800: train loss 1.5106, val loss 1.7150\n",
            "step 76900: train loss 1.5161, val loss 1.7137\n",
            "step 77000: train loss 1.5166, val loss 1.7225\n",
            "step 77100: train loss 1.5217, val loss 1.7253\n",
            "step 77200: train loss 1.5085, val loss 1.7174\n",
            "step 77300: train loss 1.5154, val loss 1.7048\n",
            "step 77400: train loss 1.5128, val loss 1.7241\n",
            "step 77500: train loss 1.5224, val loss 1.7072\n",
            "step 77600: train loss 1.5159, val loss 1.7127\n",
            "step 77700: train loss 1.5217, val loss 1.7168\n",
            "step 77800: train loss 1.5032, val loss 1.7049\n",
            "step 77900: train loss 1.5097, val loss 1.7180\n",
            "step 78000: train loss 1.5157, val loss 1.7221\n",
            "step 78100: train loss 1.5118, val loss 1.7270\n",
            "step 78200: train loss 1.5120, val loss 1.7199\n",
            "step 78300: train loss 1.5229, val loss 1.7177\n",
            "step 78400: train loss 1.5238, val loss 1.7062\n",
            "step 78500: train loss 1.5088, val loss 1.7162\n",
            "step 78600: train loss 1.5079, val loss 1.7015\n",
            "step 78700: train loss 1.5095, val loss 1.7073\n",
            "step 78800: train loss 1.5165, val loss 1.7028\n",
            "step 78900: train loss 1.5080, val loss 1.7205\n",
            "step 79000: train loss 1.5167, val loss 1.7150\n",
            "step 79100: train loss 1.5181, val loss 1.7126\n",
            "step 79200: train loss 1.5072, val loss 1.7066\n",
            "step 79300: train loss 1.5130, val loss 1.7047\n",
            "step 79400: train loss 1.5188, val loss 1.7166\n",
            "step 79500: train loss 1.5129, val loss 1.7185\n",
            "step 79600: train loss 1.5113, val loss 1.7140\n",
            "step 79700: train loss 1.5111, val loss 1.7213\n",
            "step 79800: train loss 1.5156, val loss 1.7129\n",
            "step 79900: train loss 1.5096, val loss 1.7056\n",
            "step 80000: train loss 1.5151, val loss 1.7167\n",
            "step 80100: train loss 1.5133, val loss 1.7108\n",
            "step 80200: train loss 1.5160, val loss 1.7230\n",
            "step 80300: train loss 1.5096, val loss 1.7060\n",
            "step 80400: train loss 1.5097, val loss 1.7096\n",
            "step 80500: train loss 1.5233, val loss 1.7123\n",
            "step 80600: train loss 1.5091, val loss 1.7153\n",
            "step 80700: train loss 1.5104, val loss 1.7121\n",
            "step 80800: train loss 1.5026, val loss 1.7190\n",
            "step 80900: train loss 1.5169, val loss 1.7044\n",
            "step 81000: train loss 1.5121, val loss 1.7050\n",
            "step 81100: train loss 1.5095, val loss 1.7055\n",
            "step 81200: train loss 1.5127, val loss 1.7091\n",
            "step 81300: train loss 1.5082, val loss 1.7103\n",
            "step 81400: train loss 1.5261, val loss 1.7144\n",
            "step 81500: train loss 1.5195, val loss 1.7063\n",
            "step 81600: train loss 1.5132, val loss 1.7092\n",
            "step 81700: train loss 1.5219, val loss 1.7208\n",
            "step 81800: train loss 1.5126, val loss 1.7070\n",
            "step 81900: train loss 1.5214, val loss 1.7006\n",
            "step 82000: train loss 1.5191, val loss 1.7047\n",
            "step 82100: train loss 1.5127, val loss 1.7201\n",
            "step 82200: train loss 1.5120, val loss 1.7197\n",
            "step 82300: train loss 1.5080, val loss 1.7054\n",
            "step 82400: train loss 1.5048, val loss 1.7081\n",
            "step 82500: train loss 1.5110, val loss 1.6982\n",
            "step 82600: train loss 1.5151, val loss 1.7044\n",
            "step 82700: train loss 1.5109, val loss 1.7015\n",
            "step 82800: train loss 1.5265, val loss 1.7171\n",
            "step 82900: train loss 1.5099, val loss 1.7231\n",
            "step 83000: train loss 1.5129, val loss 1.7072\n",
            "step 83100: train loss 1.5171, val loss 1.7244\n",
            "step 83200: train loss 1.5205, val loss 1.7055\n",
            "step 83300: train loss 1.5090, val loss 1.7008\n",
            "step 83400: train loss 1.5130, val loss 1.6959\n",
            "step 83500: train loss 1.5174, val loss 1.7024\n",
            "step 83600: train loss 1.5171, val loss 1.7248\n",
            "step 83700: train loss 1.5188, val loss 1.7125\n",
            "step 83800: train loss 1.5064, val loss 1.6974\n",
            "step 83900: train loss 1.5130, val loss 1.7114\n",
            "step 84000: train loss 1.5057, val loss 1.7081\n",
            "step 84100: train loss 1.5102, val loss 1.7155\n",
            "step 84200: train loss 1.5068, val loss 1.6981\n",
            "step 84300: train loss 1.5110, val loss 1.7132\n",
            "step 84400: train loss 1.5061, val loss 1.7101\n",
            "step 84500: train loss 1.5186, val loss 1.7135\n",
            "step 84600: train loss 1.5095, val loss 1.7038\n",
            "step 84700: train loss 1.5124, val loss 1.7137\n",
            "step 84800: train loss 1.5080, val loss 1.6959\n",
            "step 84900: train loss 1.5038, val loss 1.7248\n",
            "step 85000: train loss 1.5104, val loss 1.7158\n",
            "step 85100: train loss 1.5066, val loss 1.7046\n",
            "step 85200: train loss 1.5199, val loss 1.7161\n",
            "step 85300: train loss 1.5154, val loss 1.7080\n",
            "step 85400: train loss 1.5173, val loss 1.6957\n",
            "step 85500: train loss 1.5007, val loss 1.7107\n",
            "step 85600: train loss 1.5142, val loss 1.7055\n",
            "step 85700: train loss 1.5024, val loss 1.7093\n",
            "step 85800: train loss 1.5161, val loss 1.7186\n",
            "step 85900: train loss 1.5019, val loss 1.7138\n",
            "step 86000: train loss 1.5172, val loss 1.7143\n",
            "step 86100: train loss 1.5165, val loss 1.7137\n",
            "step 86200: train loss 1.5109, val loss 1.7168\n",
            "step 86300: train loss 1.5111, val loss 1.7017\n",
            "step 86400: train loss 1.5205, val loss 1.7285\n",
            "step 86500: train loss 1.5059, val loss 1.7098\n",
            "step 86600: train loss 1.5226, val loss 1.7165\n",
            "step 86700: train loss 1.5146, val loss 1.7155\n",
            "step 86800: train loss 1.5149, val loss 1.7010\n",
            "step 86900: train loss 1.5153, val loss 1.7150\n",
            "step 87000: train loss 1.5079, val loss 1.7167\n",
            "step 87100: train loss 1.5051, val loss 1.7025\n",
            "step 87200: train loss 1.5030, val loss 1.7166\n",
            "step 87300: train loss 1.5068, val loss 1.7092\n",
            "step 87400: train loss 1.5090, val loss 1.7281\n",
            "step 87500: train loss 1.4994, val loss 1.7046\n",
            "step 87600: train loss 1.5062, val loss 1.7081\n",
            "step 87700: train loss 1.5211, val loss 1.6973\n",
            "step 87800: train loss 1.5082, val loss 1.6982\n",
            "step 87900: train loss 1.5005, val loss 1.7170\n",
            "step 88000: train loss 1.5172, val loss 1.7254\n",
            "step 88100: train loss 1.5098, val loss 1.7090\n",
            "step 88200: train loss 1.5045, val loss 1.7096\n",
            "step 88300: train loss 1.5169, val loss 1.7187\n",
            "step 88400: train loss 1.5152, val loss 1.7243\n",
            "step 88500: train loss 1.5006, val loss 1.7174\n",
            "step 88600: train loss 1.5040, val loss 1.7125\n",
            "step 88700: train loss 1.5071, val loss 1.7037\n",
            "step 88800: train loss 1.5097, val loss 1.7144\n",
            "step 88900: train loss 1.4997, val loss 1.7030\n",
            "step 89000: train loss 1.5132, val loss 1.7148\n",
            "step 89100: train loss 1.5114, val loss 1.7168\n",
            "step 89200: train loss 1.5083, val loss 1.7118\n",
            "step 89300: train loss 1.4989, val loss 1.7104\n",
            "step 89400: train loss 1.5096, val loss 1.7026\n",
            "step 89500: train loss 1.5005, val loss 1.7112\n",
            "step 89600: train loss 1.5110, val loss 1.7060\n",
            "step 89700: train loss 1.5205, val loss 1.7052\n",
            "step 89800: train loss 1.5095, val loss 1.7056\n",
            "step 89900: train loss 1.4980, val loss 1.7062\n",
            "step 90000: train loss 1.5041, val loss 1.7037\n",
            "step 90100: train loss 1.5043, val loss 1.7026\n",
            "step 90200: train loss 1.4998, val loss 1.7064\n",
            "step 90300: train loss 1.5143, val loss 1.7074\n",
            "step 90400: train loss 1.5139, val loss 1.7074\n",
            "step 90500: train loss 1.5051, val loss 1.7056\n",
            "step 90600: train loss 1.5131, val loss 1.7184\n",
            "step 90700: train loss 1.5128, val loss 1.6959\n",
            "step 90800: train loss 1.4996, val loss 1.7131\n",
            "step 90900: train loss 1.5133, val loss 1.7129\n",
            "step 91000: train loss 1.5041, val loss 1.7123\n",
            "step 91100: train loss 1.5201, val loss 1.7110\n",
            "step 91200: train loss 1.5028, val loss 1.7169\n",
            "step 91300: train loss 1.5146, val loss 1.7085\n",
            "step 91400: train loss 1.5154, val loss 1.7049\n",
            "step 91500: train loss 1.5111, val loss 1.7150\n",
            "step 91600: train loss 1.5038, val loss 1.7067\n",
            "step 91700: train loss 1.5092, val loss 1.7029\n",
            "step 91800: train loss 1.5062, val loss 1.7084\n",
            "step 91900: train loss 1.5006, val loss 1.6906\n",
            "step 92000: train loss 1.5075, val loss 1.7099\n",
            "step 92100: train loss 1.5166, val loss 1.7157\n",
            "step 92200: train loss 1.5066, val loss 1.7197\n",
            "step 92300: train loss 1.5126, val loss 1.7130\n",
            "step 92400: train loss 1.5089, val loss 1.7062\n",
            "step 92500: train loss 1.5115, val loss 1.6974\n",
            "step 92600: train loss 1.5005, val loss 1.7128\n",
            "step 92700: train loss 1.5055, val loss 1.7162\n",
            "step 92800: train loss 1.5114, val loss 1.7116\n",
            "step 92900: train loss 1.5068, val loss 1.6923\n",
            "step 93000: train loss 1.5186, val loss 1.7004\n",
            "step 93100: train loss 1.5067, val loss 1.6986\n",
            "step 93200: train loss 1.5025, val loss 1.7155\n",
            "step 93300: train loss 1.4965, val loss 1.7309\n",
            "step 93400: train loss 1.5064, val loss 1.7170\n",
            "step 93500: train loss 1.5119, val loss 1.6981\n",
            "step 93600: train loss 1.5035, val loss 1.7078\n",
            "step 93700: train loss 1.5028, val loss 1.7006\n",
            "step 93800: train loss 1.4986, val loss 1.6968\n",
            "step 93900: train loss 1.5087, val loss 1.6998\n",
            "step 94000: train loss 1.5027, val loss 1.7001\n",
            "step 94100: train loss 1.5052, val loss 1.7091\n",
            "step 94200: train loss 1.4989, val loss 1.6971\n",
            "step 94300: train loss 1.5041, val loss 1.7071\n",
            "step 94400: train loss 1.5005, val loss 1.7155\n",
            "step 94500: train loss 1.4992, val loss 1.7110\n",
            "step 94600: train loss 1.5078, val loss 1.7072\n",
            "step 94700: train loss 1.5053, val loss 1.7175\n",
            "step 94800: train loss 1.5069, val loss 1.7074\n",
            "step 94900: train loss 1.4983, val loss 1.7218\n",
            "step 95000: train loss 1.5078, val loss 1.6975\n",
            "step 95100: train loss 1.4955, val loss 1.7206\n",
            "step 95200: train loss 1.5097, val loss 1.7230\n",
            "step 95300: train loss 1.5059, val loss 1.7117\n",
            "step 95400: train loss 1.5028, val loss 1.7056\n",
            "step 95500: train loss 1.5040, val loss 1.7184\n",
            "step 95600: train loss 1.4986, val loss 1.7082\n",
            "step 95700: train loss 1.5092, val loss 1.7018\n",
            "step 95800: train loss 1.5120, val loss 1.7005\n",
            "step 95900: train loss 1.5010, val loss 1.7049\n",
            "step 96000: train loss 1.4994, val loss 1.7177\n",
            "step 96100: train loss 1.5080, val loss 1.7094\n",
            "step 96200: train loss 1.5095, val loss 1.6985\n",
            "step 96300: train loss 1.5087, val loss 1.7018\n",
            "step 96400: train loss 1.5003, val loss 1.7047\n",
            "step 96500: train loss 1.5087, val loss 1.7091\n",
            "step 96600: train loss 1.5050, val loss 1.7044\n",
            "step 96700: train loss 1.5070, val loss 1.7004\n",
            "step 96800: train loss 1.5036, val loss 1.7065\n",
            "step 96900: train loss 1.5025, val loss 1.7176\n",
            "step 97000: train loss 1.5026, val loss 1.7189\n",
            "step 97100: train loss 1.5019, val loss 1.7015\n",
            "step 97200: train loss 1.5005, val loss 1.7075\n",
            "step 97300: train loss 1.5060, val loss 1.7017\n",
            "step 97400: train loss 1.5020, val loss 1.7171\n",
            "step 97500: train loss 1.5026, val loss 1.7109\n",
            "step 97600: train loss 1.4968, val loss 1.6989\n",
            "step 97700: train loss 1.5016, val loss 1.7002\n",
            "step 97800: train loss 1.5081, val loss 1.7078\n",
            "step 97900: train loss 1.5135, val loss 1.7116\n",
            "step 98000: train loss 1.4944, val loss 1.6998\n",
            "step 98100: train loss 1.5141, val loss 1.7064\n",
            "step 98200: train loss 1.5033, val loss 1.6947\n",
            "step 98300: train loss 1.5116, val loss 1.7116\n",
            "step 98400: train loss 1.4969, val loss 1.7104\n",
            "step 98500: train loss 1.5012, val loss 1.7114\n",
            "step 98600: train loss 1.5003, val loss 1.6926\n",
            "step 98700: train loss 1.4969, val loss 1.7028\n",
            "step 98800: train loss 1.5018, val loss 1.7070\n",
            "step 98900: train loss 1.4992, val loss 1.7041\n",
            "step 99000: train loss 1.5111, val loss 1.7125\n",
            "step 99100: train loss 1.5051, val loss 1.7105\n",
            "step 99200: train loss 1.5055, val loss 1.7174\n",
            "step 99300: train loss 1.4993, val loss 1.6961\n",
            "step 99400: train loss 1.5054, val loss 1.6975\n",
            "step 99500: train loss 1.5108, val loss 1.7036\n",
            "step 99600: train loss 1.5056, val loss 1.7013\n",
            "step 99700: train loss 1.5075, val loss 1.7008\n",
            "step 99800: train loss 1.4977, val loss 1.7242\n",
            "step 99900: train loss 1.5073, val loss 1.7097\n",
            "step 100000: train loss 1.4996, val loss 1.7039\n",
            "step 100100: train loss 1.5132, val loss 1.7136\n",
            "step 100200: train loss 1.5057, val loss 1.7114\n",
            "step 100300: train loss 1.5026, val loss 1.6952\n",
            "step 100400: train loss 1.5032, val loss 1.7164\n",
            "step 100500: train loss 1.5098, val loss 1.7007\n",
            "step 100600: train loss 1.4955, val loss 1.6994\n",
            "step 100700: train loss 1.5022, val loss 1.7038\n",
            "step 100800: train loss 1.5061, val loss 1.7090\n",
            "step 100900: train loss 1.5147, val loss 1.7169\n",
            "step 101000: train loss 1.4979, val loss 1.7106\n",
            "step 101100: train loss 1.5038, val loss 1.7135\n",
            "step 101200: train loss 1.5073, val loss 1.7075\n",
            "step 101300: train loss 1.4948, val loss 1.7130\n",
            "step 101400: train loss 1.5196, val loss 1.7127\n",
            "step 101500: train loss 1.4955, val loss 1.6984\n",
            "step 101600: train loss 1.5043, val loss 1.6925\n",
            "step 101700: train loss 1.5001, val loss 1.7131\n",
            "step 101800: train loss 1.4981, val loss 1.7061\n",
            "step 101900: train loss 1.5041, val loss 1.6989\n",
            "step 102000: train loss 1.5074, val loss 1.6943\n",
            "step 102100: train loss 1.5033, val loss 1.6896\n",
            "step 102200: train loss 1.5035, val loss 1.7035\n",
            "step 102300: train loss 1.5032, val loss 1.7120\n",
            "step 102400: train loss 1.5061, val loss 1.7153\n",
            "step 102500: train loss 1.5079, val loss 1.7161\n",
            "step 102600: train loss 1.5160, val loss 1.7042\n",
            "step 102700: train loss 1.5092, val loss 1.6943\n",
            "step 102800: train loss 1.5025, val loss 1.7070\n",
            "step 102900: train loss 1.5049, val loss 1.7041\n",
            "step 103000: train loss 1.5028, val loss 1.7117\n",
            "step 103100: train loss 1.4964, val loss 1.7029\n",
            "step 103200: train loss 1.5095, val loss 1.7018\n",
            "step 103300: train loss 1.5057, val loss 1.7130\n",
            "step 103400: train loss 1.5087, val loss 1.7050\n",
            "step 103500: train loss 1.5073, val loss 1.7012\n",
            "step 103600: train loss 1.5007, val loss 1.6985\n",
            "step 103700: train loss 1.5129, val loss 1.7105\n",
            "step 103800: train loss 1.5067, val loss 1.6967\n",
            "step 103900: train loss 1.5033, val loss 1.7163\n",
            "step 104000: train loss 1.5019, val loss 1.7030\n",
            "step 104100: train loss 1.4993, val loss 1.7042\n",
            "step 104200: train loss 1.4972, val loss 1.7160\n",
            "step 104300: train loss 1.5061, val loss 1.6955\n",
            "step 104400: train loss 1.5049, val loss 1.7153\n",
            "step 104500: train loss 1.5067, val loss 1.6996\n",
            "step 104600: train loss 1.5053, val loss 1.7251\n",
            "step 104700: train loss 1.5097, val loss 1.6929\n",
            "step 104800: train loss 1.5037, val loss 1.7175\n",
            "step 104900: train loss 1.5014, val loss 1.7285\n",
            "step 105000: train loss 1.5177, val loss 1.6975\n",
            "step 105100: train loss 1.5017, val loss 1.7075\n",
            "step 105200: train loss 1.5004, val loss 1.6998\n",
            "step 105300: train loss 1.4954, val loss 1.6977\n",
            "step 105400: train loss 1.4996, val loss 1.6993\n",
            "step 105500: train loss 1.5022, val loss 1.7056\n",
            "step 105600: train loss 1.4920, val loss 1.7121\n",
            "step 105700: train loss 1.4992, val loss 1.7164\n",
            "step 105800: train loss 1.5003, val loss 1.7031\n",
            "step 105900: train loss 1.5051, val loss 1.7122\n",
            "step 106000: train loss 1.4952, val loss 1.6913\n",
            "step 106100: train loss 1.5015, val loss 1.6967\n",
            "step 106200: train loss 1.5028, val loss 1.7120\n",
            "step 106300: train loss 1.5003, val loss 1.7124\n",
            "step 106400: train loss 1.5018, val loss 1.7136\n",
            "step 106500: train loss 1.5054, val loss 1.6888\n",
            "step 106600: train loss 1.5084, val loss 1.7061\n",
            "step 106700: train loss 1.4980, val loss 1.6982\n",
            "step 106800: train loss 1.5024, val loss 1.7198\n",
            "step 106900: train loss 1.4989, val loss 1.6970\n",
            "step 107000: train loss 1.5118, val loss 1.7087\n",
            "step 107100: train loss 1.4950, val loss 1.6978\n",
            "step 107200: train loss 1.5096, val loss 1.7092\n",
            "step 107300: train loss 1.4955, val loss 1.7003\n",
            "step 107400: train loss 1.4982, val loss 1.6978\n",
            "step 107500: train loss 1.5077, val loss 1.7016\n",
            "step 107600: train loss 1.5058, val loss 1.7068\n",
            "step 107700: train loss 1.5066, val loss 1.6982\n",
            "step 107800: train loss 1.4970, val loss 1.7081\n",
            "step 107900: train loss 1.5066, val loss 1.6958\n",
            "step 108000: train loss 1.5140, val loss 1.7068\n",
            "step 108100: train loss 1.4990, val loss 1.6948\n",
            "step 108200: train loss 1.5046, val loss 1.7019\n",
            "step 108300: train loss 1.4972, val loss 1.6911\n",
            "step 108400: train loss 1.5019, val loss 1.6947\n",
            "step 108500: train loss 1.4950, val loss 1.7036\n",
            "step 108600: train loss 1.4943, val loss 1.6984\n",
            "step 108700: train loss 1.5002, val loss 1.7094\n",
            "step 108800: train loss 1.4985, val loss 1.7106\n",
            "step 108900: train loss 1.4940, val loss 1.7000\n",
            "step 109000: train loss 1.5078, val loss 1.7087\n",
            "step 109100: train loss 1.5049, val loss 1.7130\n",
            "step 109200: train loss 1.4985, val loss 1.6958\n",
            "step 109300: train loss 1.4959, val loss 1.6917\n",
            "step 109400: train loss 1.4945, val loss 1.7113\n",
            "step 109500: train loss 1.5016, val loss 1.7134\n",
            "step 109600: train loss 1.4987, val loss 1.6967\n",
            "step 109700: train loss 1.5031, val loss 1.6943\n",
            "step 109800: train loss 1.5034, val loss 1.7095\n",
            "step 109900: train loss 1.4976, val loss 1.7046\n",
            "step 110000: train loss 1.4975, val loss 1.7026\n",
            "step 110100: train loss 1.4939, val loss 1.7021\n",
            "step 110200: train loss 1.5070, val loss 1.6931\n",
            "step 110300: train loss 1.5023, val loss 1.6874\n",
            "step 110400: train loss 1.5009, val loss 1.6825\n",
            "step 110500: train loss 1.5057, val loss 1.6962\n",
            "step 110600: train loss 1.5120, val loss 1.6963\n",
            "step 110700: train loss 1.5000, val loss 1.6860\n",
            "step 110800: train loss 1.5038, val loss 1.6947\n",
            "step 110900: train loss 1.4963, val loss 1.7136\n",
            "step 111000: train loss 1.4909, val loss 1.6934\n",
            "step 111100: train loss 1.5086, val loss 1.6995\n",
            "step 111200: train loss 1.4969, val loss 1.6985\n",
            "step 111300: train loss 1.4935, val loss 1.6996\n",
            "step 111400: train loss 1.4891, val loss 1.7168\n",
            "step 111500: train loss 1.5045, val loss 1.7086\n",
            "step 111600: train loss 1.5064, val loss 1.7127\n",
            "step 111700: train loss 1.4978, val loss 1.6894\n",
            "step 111800: train loss 1.5008, val loss 1.6795\n",
            "step 111900: train loss 1.5057, val loss 1.6987\n",
            "step 112000: train loss 1.4997, val loss 1.7009\n",
            "step 112100: train loss 1.4886, val loss 1.7108\n",
            "step 112200: train loss 1.4977, val loss 1.6921\n",
            "step 112300: train loss 1.5073, val loss 1.7034\n",
            "step 112400: train loss 1.5016, val loss 1.7101\n",
            "step 112500: train loss 1.5054, val loss 1.7143\n",
            "step 112600: train loss 1.4984, val loss 1.6953\n",
            "step 112700: train loss 1.4955, val loss 1.7019\n",
            "step 112800: train loss 1.4973, val loss 1.7036\n",
            "step 112900: train loss 1.4932, val loss 1.7065\n",
            "step 113000: train loss 1.4953, val loss 1.7041\n",
            "step 113100: train loss 1.4913, val loss 1.6946\n",
            "step 113200: train loss 1.4936, val loss 1.7035\n",
            "step 113300: train loss 1.4966, val loss 1.6998\n",
            "step 113400: train loss 1.4958, val loss 1.6927\n",
            "step 113500: train loss 1.4947, val loss 1.7124\n",
            "step 113600: train loss 1.4988, val loss 1.6951\n",
            "step 113700: train loss 1.4978, val loss 1.6905\n",
            "step 113800: train loss 1.4980, val loss 1.6949\n",
            "step 113900: train loss 1.5001, val loss 1.7024\n",
            "step 114000: train loss 1.4991, val loss 1.6987\n",
            "step 114100: train loss 1.5077, val loss 1.7075\n",
            "step 114200: train loss 1.5003, val loss 1.7046\n",
            "step 114300: train loss 1.5006, val loss 1.6952\n",
            "step 114400: train loss 1.5096, val loss 1.7116\n",
            "step 114500: train loss 1.4970, val loss 1.7123\n",
            "step 114600: train loss 1.4934, val loss 1.7012\n",
            "step 114700: train loss 1.4995, val loss 1.7127\n",
            "step 114800: train loss 1.4960, val loss 1.7103\n",
            "step 114900: train loss 1.5067, val loss 1.7065\n",
            "step 115000: train loss 1.4951, val loss 1.6979\n",
            "step 115100: train loss 1.5031, val loss 1.6930\n",
            "step 115200: train loss 1.4957, val loss 1.7033\n",
            "step 115300: train loss 1.5047, val loss 1.7060\n",
            "step 115400: train loss 1.4980, val loss 1.7127\n",
            "step 115500: train loss 1.5004, val loss 1.7038\n",
            "step 115600: train loss 1.4859, val loss 1.7155\n",
            "step 115700: train loss 1.5018, val loss 1.7126\n",
            "step 115800: train loss 1.4968, val loss 1.7198\n",
            "step 115900: train loss 1.5017, val loss 1.6956\n",
            "step 116000: train loss 1.4963, val loss 1.6978\n",
            "step 116100: train loss 1.4976, val loss 1.6977\n",
            "step 116200: train loss 1.4950, val loss 1.7121\n",
            "step 116300: train loss 1.4888, val loss 1.6899\n",
            "step 116400: train loss 1.5001, val loss 1.6990\n",
            "step 116500: train loss 1.4987, val loss 1.6996\n",
            "step 116600: train loss 1.5089, val loss 1.7025\n",
            "step 116700: train loss 1.5012, val loss 1.7087\n",
            "step 116800: train loss 1.4962, val loss 1.6905\n",
            "step 116900: train loss 1.4911, val loss 1.7066\n",
            "step 117000: train loss 1.4984, val loss 1.7010\n",
            "step 117100: train loss 1.5032, val loss 1.7084\n",
            "step 117200: train loss 1.5058, val loss 1.6902\n",
            "step 117300: train loss 1.4963, val loss 1.6990\n",
            "step 117400: train loss 1.5001, val loss 1.7049\n",
            "step 117500: train loss 1.5051, val loss 1.7098\n",
            "step 117600: train loss 1.4960, val loss 1.6824\n",
            "step 117700: train loss 1.4914, val loss 1.6898\n",
            "step 117800: train loss 1.4930, val loss 1.7125\n",
            "step 117900: train loss 1.4992, val loss 1.7185\n",
            "step 118000: train loss 1.5055, val loss 1.7147\n",
            "step 118100: train loss 1.4948, val loss 1.7111\n",
            "step 118200: train loss 1.4861, val loss 1.6954\n",
            "step 118300: train loss 1.4934, val loss 1.6998\n",
            "step 118400: train loss 1.4941, val loss 1.7126\n",
            "step 118500: train loss 1.4954, val loss 1.7179\n",
            "step 118600: train loss 1.4941, val loss 1.6923\n",
            "step 118700: train loss 1.5016, val loss 1.7140\n",
            "step 118800: train loss 1.4960, val loss 1.6976\n",
            "step 118900: train loss 1.4948, val loss 1.6949\n",
            "step 119000: train loss 1.4969, val loss 1.6994\n",
            "step 119100: train loss 1.4992, val loss 1.6961\n",
            "step 119200: train loss 1.4988, val loss 1.6907\n",
            "step 119300: train loss 1.4990, val loss 1.7006\n",
            "step 119400: train loss 1.4991, val loss 1.7075\n",
            "step 119500: train loss 1.4990, val loss 1.7209\n",
            "step 119600: train loss 1.4998, val loss 1.7120\n",
            "step 119700: train loss 1.5044, val loss 1.7137\n",
            "step 119800: train loss 1.5002, val loss 1.7033\n",
            "step 119900: train loss 1.4905, val loss 1.6958\n",
            "step 119999: train loss 1.4971, val loss 1.7001\n",
            "\tgando-me?\n",
            "\n",
            " Talvez outra é.\n",
            "\n",
            "Uma fim deliberar.\n",
            "\n",
            "O doente às suas mais cotejares;\n",
            "\n",
            " Julião fiquei o afeturo de uma escolher à sala, mas a princípios\n",
            "na traça douta e natural, ela senhora, ao subinal encerrável mal, a moça só junhos\n",
            "  largos à influência. cuja já que vou uma mulher dos caráteres da pública, as m...\n",
            "\n",
            "(Emlete.\n",
            "\n",
            "Eram sinal abismos tamanhos resoluçãos homens à justa\n",
            "doutra, deixando nem só a mim; se lhe procurava de primeiro carreiro\n",
            "      detro, vejo e\n",
            "escutário? Cultivo o\n",
            "uso decimento, pediu-nos, ia salvador. Pouco era com os moços agoram, mas a alternada a\n",
            "incompensária exterior em trazer longo'e\n",
            "alguns juízos alguns dedos; agora a sensição e levado na Senhora. Quanto afigurou:\n",
            "\n",
            "?\n",
            "\n",
            "Forte estou... O Sr. Galela,\n",
            "\n",
            "Luísa estas ditos pareceu ao enfada, a pálina\n",
            "    das nossas à lei do amparável,\n",
            "menos os dedos, abenços e a outra não a.\n",
            "E vê-lo aqui coisas? Conterançava\n",
            "por uma boiguosa; ambos depois dos médicos, José\n",
            " virtude em veio\n",
            "justar às mulheres\n",
            "e casa o contado sem sombrinho, acaso, e os filos do filósofo públique.\n",
            "\n",
            "Foi as nove...\n",
            "\n",
            " Bernas vi a esse amigo, gonçou procurar, porém, e\n",
            "buiria esta razão no tempo, mas que sagora4\n",
            "\n",
            "Deus nestenam vontade, Sinto; agora no algar lá se examebiu seadro e eis alguma.\n",
            "\n",
            "A mesmo que nada\n",
            "advogada, e qualquer liberal, e descuidada, lobo e que me não sonha, que tu orro. Não\n",
            "era seu sentimento devia\n",
            "cercaste,\n",
            "\n",
            "A viúva um\n",
            "mostrado; mas direto tinha março. O baronesto de Estante de José Canto. Já hábito, sem suspelho do ilho, mais tão maria trezer a deixá-lo do vaolunate a eserguidade. Benedó, muito antes da vida mesma.\n",
            "\n",
            "De certo e pelegislacista para morrer ao espírito, que a casa natura, à moça pouco! Ultimo. Um simples, não sou memas, a\n",
            "cerimônia de\n",
            "todo. Creio; a comisessões inglativamente para outros que esteve alegria nada; mas se ausia, ele sabe a\n",
            "Tantar que ele operava das deus apesar do meu chapéu. E não ainda uma tever por ele, mas peitei dera; a íntima, só tranqüilos pales dóstimas no caminho da viúva; \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN_oCyBCOmkz",
        "outputId": "163d8539-2359-4d4c-b7ea-3a630675f0d5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tingulamente.\n",
            "\n",
            " Parecia daquela história.\n",
            "\n",
            "Resisteve não a\n",
            "  vista\n",
            "enfermar\n",
            "a Preladura teveu poder, uso; entre a que a hiês insultante que sobre a terra que experiência de um longe:  Mas\n",
            "desceu meques; confessos\n",
            "andares os dois...\n",
            "\n",
            "-- Não lá se compõe em satisfance. A verdade,\n",
            "agora entendeu a maral e criança...\n",
            "\n",
            "As diplomáticas d'....\n",
            "\n",
            "         Desse sunho? E\n",
            "     maior nenhuma, apmassado então. Não\n",
            "posso que ânimo foi\n",
            "estes estrelam-se e as divinais deixoundo. Tal\n",
            "  relação secreto, que o\n",
            "solículo sorria, em cena nela amanhã...\n",
            "pensando o meio dos sentimentos e vós, aos\n",
            "sobretudos sorriu-se deu-lhe ao colegas).\n",
            "\n",
            " É dispos,\n",
            "jovens exemplar como o mais mais sócinos espécies apreciada. Trazia ela\n",
            "que oito dizer que o passo que pode adoecer na pretendente e de morto, nada quinzena: é dia dois da designificar...\n",
            "\n",
            "Deus se sentara vergonhas e rapleens e anos espereceu os olcantos à mania, que você disseram com os estremeceu assim dois tamos de nencentes. Que\n",
            "  mandaria? perguntei ela.\n",
            "\n",
            "Valoganis.\n",
            "\n",
            " Uma que ia mobar a toilenta sofrinha felizes ou aconteceu ânimo\n",
            "e dois dias, há almama! Mas a zimolapidade\n",
            "ésperava o espancol de que não era um filho; Paula madrinho, e pela igualidade e minhou o primeiro. Ele\n",
            "ali mão, mais entretanto exatos de laços\n",
            "\n",
            "Valéria de gastar o livro a\n",
            "leite do existente, e levou a política.\n",
            "\n",
            "TIRO DE\n",
            "1871.\n",
            "\n",
            "Aquele impensar.\n",
            "\n",
            "E satisfeito, que tenho uma\n",
            "plancidade para sinal única. Surto triiçava às Resistinas da Fernandblines tão\n",
            "  deixaremos o moleque não o prazo não te acaba, mas os áculos galos, Ouso que fui\n",
            "milandas baixões são meus digos; ia por ê-lo primeiro crítico da beleza.\n",
            "\n",
            "A Quinhenta é um sonetante Sonóio ELÂBA\n",
            "mandes então, há muitos; aceitando como\n",
            "ela tinha, deotes as duesons\n",
            "\n",
            "Texto-feivor dos homens, os dois altos do esboçário.\n",
            "\n",
            " Mas referiu, porém ser aceitar  saiu em conveniência. Não serão! a\n",
            "boceira.\n",
            "\n",
            "A impaciência da casama do Mince. Pedia-lhe a fugalar graça\n",
            "  Horálice, não era sido baixo que fazem das mãos aprossessas. Ba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "ebAhRAjKOp5s"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lxLCn8vt9kmJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}